{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "995d74bb",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e1647d7f",
   "metadata": {},
   "source": [
    "# 1.tokenizer,æ„é€ è¾“å…¥"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98261122",
   "metadata": {},
   "source": [
    "- tokenizer, model :ç›¸åŒ¹é…ï¼Œtokenizer outputs  => model input\n",
    "- Auto*Tokenizer,Auto*Model: Generic type,Autoå¼€å¤´çš„ä¼šè‡ªé€‚åº”çš„æ‰¾åˆ°å¯¹åº”çš„modelå’ŒTokenizer,æœåŠ¡äºmodel input\n",
    "    - len(input_ids) == len(attention_mask)\n",
    "    - tokenizer(test_senteces[0],) ==> tokenizer.\\_\\_call\\_\\_ ==>encode\n",
    "    - tokenizer.encode çº¦ç­‰äº tokenizer.tokenizeæ¬¡å…ƒ + tokenizer.convert_tokens_to_ids idï¼›ç¼ºå°‘èµ·æ­¢æ ‡è¯†ç¬¦101ï¼Œ102\n",
    "    - tokenizer.decode \n",
    "    - tokenizer çš„å·¥ä½œåŸç†ä¸»è¦ä¾èµ–äºtokenizer.vocabå­—å…¸å­˜å‚¨äº†tokens ==> ids çš„æ˜ å°„å…³ç³»\n",
    "        - tokenizer.special_tokens_map è®°å½•äº†ä¸€äº›ç‰¹æ®Šç¬¦å· ==> tokens\n",
    "    - attenmask = 1 çš„éƒ¨åˆ†ä¸ºå¥å­åŸå§‹éƒ¨åˆ†ï¼Œattemask = 0 çš„éƒ¨åˆ†æŒ‡çš„æ—¶paddingéƒ¨åˆ† \n",
    "    - truncation ä¸ max_length é…åˆä¸€èµ·ä½œç”¨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8e7d2158",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_senteces = ['today is not that bad', 'today is so bad']\n",
    "model_name = 'distilbert-base-uncased-finetuned-sst-2-english'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2eea83b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "50f1d2d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c776ef4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ea1e00078e14f58a1eb2c0614555d03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/629 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\juju\\AppData\\Roaming\\Python\\Python312\\site-packages\\huggingface_hub\\file_download.py:130: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\juju\\.cache\\huggingface\\hub\\models--distilbert-base-uncased-finetuned-sst-2-english. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22ee40e16885470e94bcc1c684ecbf39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d73ad4075344527aa630955d701b89c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e3af49b8d3f479296e0b657a8006e38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fbfb737c054440f098fa1f1216a0a3a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/104 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c878a7a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_input = tokenizer(test_senteces, truncation = True,  padding=True,return_tensors='pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8c9080f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[ 101, 2651, 2003, 2025, 2008, 2919,  102],\n",
       "        [ 101, 2651, 2003, 2061, 2919,  102,    0]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 0]])}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "339fd085",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['today', 'is', 'not', 'that', 'bad']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize(test_senteces[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bd91a85b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2651, 2003, 2025, 2008, 2919]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_tokens_to_ids(tokenizer.tokenize(test_senteces[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "79e963f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[101, 2651, 2003, 2025, 2008, 2919, 102]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(test_senteces[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0a897991",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 2651, 2003, 2025, 2008, 2919, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(test_senteces[0],)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ce4afe13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] today is not that bad [SEP]'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode([101, 2651, 2003, 2025, 2008, 2919, 102])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3669743b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'[unused805]': 810,\n",
       " 'skylar': 27970,\n",
       " 'revealing': 8669,\n",
       " 'tractor': 16358,\n",
       " '##ways': 14035,\n",
       " 'eliza': 13234,\n",
       " 'gift': 5592,\n",
       " 'á„‘': 1468,\n",
       " '##kov': 7724,\n",
       " 'gardner': 11764,\n",
       " 'rewarded': 14610,\n",
       " 'practical': 6742,\n",
       " 'shelter': 7713,\n",
       " 'doorway': 7086,\n",
       " 'definitive': 15764,\n",
       " 'coefficients': 21374,\n",
       " 'supervise': 28589,\n",
       " '1830s': 20400,\n",
       " '[unused839]': 844,\n",
       " '##town': 4665,\n",
       " 'buena': 27493,\n",
       " 'diver': 17856,\n",
       " 'mean': 2812,\n",
       " 'sculptors': 28417,\n",
       " 'magnolia': 24659,\n",
       " '##nosis': 27109,\n",
       " 'beside': 3875,\n",
       " '1702': 26776,\n",
       " 'installations': 14111,\n",
       " '##eral': 21673,\n",
       " 'psalm': 22728,\n",
       " '##ever': 22507,\n",
       " '##uring': 12228,\n",
       " 've': 2310,\n",
       " '##gel': 12439,\n",
       " 'plume': 26888,\n",
       " 'absolute': 7619,\n",
       " 'church': 2277,\n",
       " 'director': 2472,\n",
       " 'was': 2001,\n",
       " 'sired': 26940,\n",
       " 'chuckled': 10252,\n",
       " 'hired': 5086,\n",
       " '##à§‡': 29917,\n",
       " 'croatia': 8097,\n",
       " '##uni': 19496,\n",
       " 'resorts': 16511,\n",
       " 'daylight': 11695,\n",
       " 'behaviour': 9164,\n",
       " 'apostolic': 11815,\n",
       " '31': 2861,\n",
       " '##aka': 11905,\n",
       " 'abnormal': 19470,\n",
       " '##ç§€': 30454,\n",
       " '##lke': 28143,\n",
       " 'carroll': 10767,\n",
       " 'truss': 24224,\n",
       " '##mbling': 29256,\n",
       " 'kw': 6448,\n",
       " 'spp': 26924,\n",
       " 'excluding': 13343,\n",
       " '##fort': 13028,\n",
       " 'worry': 4737,\n",
       " 'sonata': 14681,\n",
       " '##ried': 11998,\n",
       " 'prove': 6011,\n",
       " 'plunging': 29059,\n",
       " 'trenton': 17148,\n",
       " '##pled': 21132,\n",
       " 'scheduling': 19940,\n",
       " '##red': 5596,\n",
       " '##ifice': 23664,\n",
       " 'prehistoric': 14491,\n",
       " 'obtaining': 11381,\n",
       " 'must': 2442,\n",
       " '##à¦¤': 29898,\n",
       " '##log': 21197,\n",
       " 'mapped': 17715,\n",
       " 'reigns': 23481,\n",
       " 'spectator': 21027,\n",
       " '##|': 29641,\n",
       " '##roud': 21332,\n",
       " 'annum': 28907,\n",
       " 'chatting': 22331,\n",
       " 'split': 3975,\n",
       " '##bbs': 17226,\n",
       " 'becoming': 3352,\n",
       " 'cheyenne': 17778,\n",
       " 'uttered': 20947,\n",
       " 'louder': 10989,\n",
       " 'viii': 9937,\n",
       " 'ached': 15043,\n",
       " 'infected': 10372,\n",
       " 'karate': 16894,\n",
       " 'ware': 16283,\n",
       " 'hospitality': 15961,\n",
       " 'pontifical': 22362,\n",
       " 'tough': 7823,\n",
       " 'balthazar': 25021,\n",
       " 'wren': 16255,\n",
       " 'sped': 16887,\n",
       " 'maternal': 11062,\n",
       " 'shout': 11245,\n",
       " 'relied': 13538,\n",
       " '##yoshi': 19196,\n",
       " 'cardiff': 10149,\n",
       " 'movements': 5750,\n",
       " 'sprawled': 21212,\n",
       " '[unused55]': 56,\n",
       " 'fading': 14059,\n",
       " 'â€–': 1519,\n",
       " 'fore': 18921,\n",
       " 'cis': 20199,\n",
       " 'luce': 19913,\n",
       " 'libertarian': 19297,\n",
       " 'faso': 22773,\n",
       " '##gui': 25698,\n",
       " 'broader': 12368,\n",
       " 'waitress': 13877,\n",
       " 'manipulation': 16924,\n",
       " 'neglect': 19046,\n",
       " 'crying': 6933,\n",
       " 'undergoing': 14996,\n",
       " '##jou': 23099,\n",
       " 'rave': 23289,\n",
       " 'excitedly': 23885,\n",
       " 'copyright': 9385,\n",
       " 'mutually': 20271,\n",
       " '##note': 22074,\n",
       " 'cloudy': 24706,\n",
       " 'nazareth': 27192,\n",
       " 'swam': 16849,\n",
       " 'utrecht': 18361,\n",
       " 'phosphorus': 25473,\n",
       " '##ste': 13473,\n",
       " 'stump': 22475,\n",
       " 'formally': 6246,\n",
       " 'u2': 23343,\n",
       " 'scoffed': 26326,\n",
       " 'bartender': 15812,\n",
       " 'contests': 15795,\n",
       " '##sÅ‚aw': 23305,\n",
       " 'ochreous': 23474,\n",
       " 'à¦¨': 1366,\n",
       " 'landed': 5565,\n",
       " 'padua': 24941,\n",
       " 'eyelashes': 25150,\n",
       " 'cain': 11557,\n",
       " 'stares': 14020,\n",
       " '##à¯ˆ': 29935,\n",
       " '##ra': 2527,\n",
       " 'mango': 24792,\n",
       " 'oblique': 20658,\n",
       " 'publicly': 7271,\n",
       " 'warped': 25618,\n",
       " 'mainly': 3701,\n",
       " '##ilised': 21758,\n",
       " 'richter': 20105,\n",
       " '[unused562]': 567,\n",
       " 'kenyan': 20428,\n",
       " 'caressing': 25296,\n",
       " 'nets': 16996,\n",
       " 'é‡Œ': 1962,\n",
       " 'exceed': 13467,\n",
       " 'banking': 8169,\n",
       " 'burnham': 25295,\n",
       " 'cinematographer': 19245,\n",
       " 'montreal': 5548,\n",
       " '##ock': 7432,\n",
       " 'them': 2068,\n",
       " 'struck': 4930,\n",
       " 'consequences': 8465,\n",
       " 'fairies': 20182,\n",
       " 'sincerity': 23997,\n",
       " 'dimitri': 15953,\n",
       " 'asteroid': 12175,\n",
       " 'shareholders': 15337,\n",
       " 'daring': 15236,\n",
       " 'rejecting': 21936,\n",
       " 'goddard': 22547,\n",
       " 'freight': 8441,\n",
       " 'csi': 22174,\n",
       " 'dynasties': 23014,\n",
       " 'invention': 11028,\n",
       " '##zia': 12871,\n",
       " 'acre': 7456,\n",
       " 'australians': 15739,\n",
       " 'shivered': 13927,\n",
       " 'bewildered': 24683,\n",
       " '##ille': 10484,\n",
       " 'turning': 3810,\n",
       " '##ã¯': 30198,\n",
       " 'songwriting': 14029,\n",
       " 'my': 2026,\n",
       " 'foundry': 19853,\n",
       " '##pic': 24330,\n",
       " 'suspension': 8636,\n",
       " 'dvd': 4966,\n",
       " '##wg': 27767,\n",
       " 'righteous': 19556,\n",
       " 'papa': 13008,\n",
       " 'moritz': 28461,\n",
       " 'genoa': 15771,\n",
       " 'tournament': 2977,\n",
       " 'naturalized': 27558,\n",
       " 'upcoming': 9046,\n",
       " 'barron': 23594,\n",
       " 'convenient': 14057,\n",
       " 'milling': 22491,\n",
       " '##nese': 14183,\n",
       " 'sooner': 10076,\n",
       " 'ambitious': 12479,\n",
       " 'dodd': 21258,\n",
       " 'commitments': 17786,\n",
       " '##reen': 28029,\n",
       " 'varieties': 9903,\n",
       " '##sburg': 9695,\n",
       " 'interact': 11835,\n",
       " 'topical': 25665,\n",
       " 'popping': 20095,\n",
       " 'posthumous': 19086,\n",
       " 'frankie': 12784,\n",
       " 'measures': 5761,\n",
       " 'gallons': 18501,\n",
       " '##lence': 22717,\n",
       " 'reporting': 7316,\n",
       " 'graphical': 20477,\n",
       " 'regents': 22832,\n",
       " 'deployed': 7333,\n",
       " 'clans': 16411,\n",
       " 'roe': 20944,\n",
       " 'funeral': 6715,\n",
       " 'renovations': 15576,\n",
       " 'nerves': 10627,\n",
       " 'sport': 4368,\n",
       " 'bones': 5944,\n",
       " 'incoming': 14932,\n",
       " 'torque': 15894,\n",
       " '##pipe': 24548,\n",
       " 'cigarette': 9907,\n",
       " 'ligand': 27854,\n",
       " 'emory': 25853,\n",
       " 'consensus': 10465,\n",
       " '##ima': 9581,\n",
       " 'miniatures': 28615,\n",
       " 'weekends': 13499,\n",
       " 'consume': 16678,\n",
       " 'rated': 6758,\n",
       " 'craig': 7010,\n",
       " '##hawks': 16043,\n",
       " '##eros': 27360,\n",
       " 'upgrade': 12200,\n",
       " 'slot': 10453,\n",
       " 'espionage': 21003,\n",
       " 'motive': 15793,\n",
       " '[unused534]': 539,\n",
       " 'interstate': 7553,\n",
       " 'haunt': 24542,\n",
       " '[unused305]': 310,\n",
       " 'radiant': 23751,\n",
       " 'factions': 13815,\n",
       " 'capabilities': 9859,\n",
       " 'sellers': 19041,\n",
       " 'ron': 6902,\n",
       " 'momentary': 29089,\n",
       " 'uncles': 27328,\n",
       " '[unused712]': 717,\n",
       " 'É¡': 1116,\n",
       " '1626': 28818,\n",
       " 'everett': 15160,\n",
       " 'partnerships': 13797,\n",
       " 'highlands': 11784,\n",
       " 'noon': 11501,\n",
       " 'vertigo': 28246,\n",
       " 'ãƒƒ': 1711,\n",
       " '[unused671]': 676,\n",
       " '=': 1027,\n",
       " 'refinery': 21034,\n",
       " 'moderator': 29420,\n",
       " '##!': 29612,\n",
       " '##â€š': 30057,\n",
       " '[unused224]': 229,\n",
       " 'starved': 26042,\n",
       " 'usually': 2788,\n",
       " 'defective': 28829,\n",
       " 'fooled': 25857,\n",
       " 'grip': 6218,\n",
       " 'strasbourg': 18104,\n",
       " 'ã€ˆ': 1637,\n",
       " 'challenges': 7860,\n",
       " '##sphere': 23874,\n",
       " '1891': 6607,\n",
       " 'exemption': 19621,\n",
       " 'pardon': 14933,\n",
       " 'saxe': 24937,\n",
       " '##ll': 3363,\n",
       " 'profession': 9518,\n",
       " 'tens': 15295,\n",
       " 'soviet': 3354,\n",
       " 'rebuild': 14591,\n",
       " 'ge': 16216,\n",
       " 'fraser': 9443,\n",
       " 'spirits': 8633,\n",
       " 'oil': 3514,\n",
       " 'arguments': 9918,\n",
       " '137': 14989,\n",
       " 'berkeley': 8256,\n",
       " 'havre': 28890,\n",
       " 'cliffs': 13333,\n",
       " 'hansen': 13328,\n",
       " '##berger': 14859,\n",
       " '##kk': 19658,\n",
       " 'stimulated': 25194,\n",
       " 'writings': 7896,\n",
       " 'variations': 8358,\n",
       " 'shriek': 24795,\n",
       " '[unused306]': 311,\n",
       " 'hectares': 9076,\n",
       " 'spherical': 18970,\n",
       " 'denim': 26762,\n",
       " 'indo': 11424,\n",
       " 'rhys': 13919,\n",
       " 'à¦°': 1372,\n",
       " 'prolific': 12807,\n",
       " '##stock': 14758,\n",
       " 'swept': 7260,\n",
       " 'detailing': 17555,\n",
       " 'hush': 20261,\n",
       " '##dos': 12269,\n",
       " 'distinctions': 25995,\n",
       " 'noticeably': 25327,\n",
       " 'bolts': 19947,\n",
       " 'embodied': 25405,\n",
       " '##igate': 28731,\n",
       " 'pollard': 25513,\n",
       " 'that': 2008,\n",
       " 'jade': 12323,\n",
       " 'stillness': 29435,\n",
       " '##rith': 24292,\n",
       " '101': 7886,\n",
       " 'undone': 25757,\n",
       " 'records': 2636,\n",
       " 'halt': 9190,\n",
       " '[unused922]': 927,\n",
       " '##âŸ©': 30156,\n",
       " 'jays': 18930,\n",
       " '##ysis': 20960,\n",
       " 'tub': 14366,\n",
       " 'minogue': 27736,\n",
       " 'beloved': 11419,\n",
       " '##ulate': 9869,\n",
       " '##åŠ›': 30304,\n",
       " '700': 6352,\n",
       " 'zimbabwe': 11399,\n",
       " 'porto': 13809,\n",
       " 'implies': 12748,\n",
       " 'reserves': 8269,\n",
       " 'simplest': 21304,\n",
       " 'analysts': 18288,\n",
       " 'modelled': 23364,\n",
       " 'consecutive': 5486,\n",
       " 'indefinite': 25617,\n",
       " 'constitutes': 17367,\n",
       " 'engine': 3194,\n",
       " 'chichester': 23406,\n",
       " 'salmon': 11840,\n",
       " '##tangled': 27898,\n",
       " '##min': 10020,\n",
       " '##wood': 3702,\n",
       " 'ethan': 6066,\n",
       " 'given': 2445,\n",
       " 'gauge': 7633,\n",
       " 'cp': 18133,\n",
       " 'connell': 17199,\n",
       " 'folk': 5154,\n",
       " 'injuries': 6441,\n",
       " 'unavailable': 20165,\n",
       " 'cuisine': 12846,\n",
       " 'ankle': 10792,\n",
       " 'clapped': 18310,\n",
       " 'competes': 14190,\n",
       " 'surplus': 15726,\n",
       " 'axel': 18586,\n",
       " 'fran': 23151,\n",
       " '##blood': 26682,\n",
       " 'dmitri': 28316,\n",
       " '1727': 25350,\n",
       " 'residing': 7154,\n",
       " 'ten': 2702,\n",
       " 'fallen': 5357,\n",
       " '##lift': 18412,\n",
       " 'autumn': 7114,\n",
       " '##rton': 11715,\n",
       " '##lining': 16992,\n",
       " 'jewels': 15565,\n",
       " '##pop': 16340,\n",
       " 'æ‘': 1878,\n",
       " 'commanding': 7991,\n",
       " '##â€²': 30066,\n",
       " 'hoc': 21929,\n",
       " 'tightened': 8371,\n",
       " 'cousin': 5542,\n",
       " 'hips': 6700,\n",
       " 'culinary': 20560,\n",
       " 'mathews': 23287,\n",
       " 'absorbing': 20998,\n",
       " 'nazis': 13157,\n",
       " '730': 28004,\n",
       " 'big': 2502,\n",
       " 'probe': 15113,\n",
       " 'meiji': 23214,\n",
       " 'career': 2476,\n",
       " '##riety': 27840,\n",
       " 'lenin': 17497,\n",
       " '[unused27]': 28,\n",
       " 'auditioned': 23008,\n",
       " '##âˆª': 30132,\n",
       " '##orous': 25373,\n",
       " 'fearing': 14892,\n",
       " '##plex': 19386,\n",
       " 'foundation': 3192,\n",
       " 'fallout': 23902,\n",
       " 'corporate': 5971,\n",
       " 'freezing': 12809,\n",
       " 'joanne': 23459,\n",
       " '##rative': 18514,\n",
       " '##khar': 22510,\n",
       " 'bn': 24869,\n",
       " 'gangster': 20067,\n",
       " 'norway': 5120,\n",
       " 'likelihood': 16593,\n",
       " '##rling': 22036,\n",
       " 'blows': 13783,\n",
       " 'bravery': 16534,\n",
       " 'ebert': 22660,\n",
       " '##tery': 20902,\n",
       " 'neighbours': 14754,\n",
       " 'sphere': 10336,\n",
       " '##oric': 29180,\n",
       " 'bike': 7997,\n",
       " 'processing': 6364,\n",
       " 'textures': 29343,\n",
       " 'salts': 23480,\n",
       " 'pregnant': 6875,\n",
       " 'groove': 14100,\n",
       " 'masjid': 27779,\n",
       " 'detroit': 5626,\n",
       " 'cheng': 15898,\n",
       " 'discusses': 15841,\n",
       " 'nothin': 24218,\n",
       " 'revenue': 6599,\n",
       " 'allowing': 4352,\n",
       " '[unused516]': 521,\n",
       " 'aside': 4998,\n",
       " 'reunion': 10301,\n",
       " 'threshold': 11207,\n",
       " 'remake': 12661,\n",
       " 'refined': 15514,\n",
       " 'higgins': 13466,\n",
       " 'ã‚’': 1690,\n",
       " 'cooks': 26929,\n",
       " 'between': 2090,\n",
       " '##men': 3549,\n",
       " 'norwegian': 5046,\n",
       " 'physiology': 16127,\n",
       " 'notebook': 14960,\n",
       " 'door': 2341,\n",
       " 'hasty': 27151,\n",
       " 'amin': 24432,\n",
       " 'boulders': 22177,\n",
       " '##sho': 22231,\n",
       " 'claire': 6249,\n",
       " '##nr': 16118,\n",
       " '[unused444]': 449,\n",
       " 'Ñ‰': 1204,\n",
       " 'discs': 15303,\n",
       " '268': 25143,\n",
       " 'gut': 9535,\n",
       " 'generic': 12391,\n",
       " 'hasan': 17000,\n",
       " 'painted': 4993,\n",
       " 'thessaloniki': 23162,\n",
       " '##iche': 17322,\n",
       " 'flesh': 5771,\n",
       " 'briefs': 28760,\n",
       " 'affinity': 16730,\n",
       " '##sen': 5054,\n",
       " '##cb': 27421,\n",
       " 'symphonic': 18957,\n",
       " 'medicare': 27615,\n",
       " 'assess': 14358,\n",
       " 'construct': 9570,\n",
       " 'renegade': 28463,\n",
       " 'austin': 5899,\n",
       " 'spurred': 22464,\n",
       " 'lest': 26693,\n",
       " 'fractures': 28929,\n",
       " 'trainer': 10365,\n",
       " 'unnecessary': 14203,\n",
       " 'investigation': 4812,\n",
       " '[unused659]': 664,\n",
       " 'lee': 3389,\n",
       " 'puebla': 27452,\n",
       " 'instructors': 19922,\n",
       " 'grossed': 17500,\n",
       " 'opposite': 4500,\n",
       " 'drinks': 8974,\n",
       " 'dimensional': 8789,\n",
       " '##sner': 20479,\n",
       " '[unused849]': 854,\n",
       " 'frigate': 15437,\n",
       " 'â™¥': 1625,\n",
       " 'diner': 15736,\n",
       " '##twined': 21077,\n",
       " '##rah': 10404,\n",
       " 'verde': 16184,\n",
       " '1872': 7572,\n",
       " '##med': 7583,\n",
       " '##dak': 23597,\n",
       " 'extremes': 28800,\n",
       " 'susan': 6294,\n",
       " '##nitz': 22792,\n",
       " '##eke': 23941,\n",
       " 'box': 3482,\n",
       " 'disrupt': 23217,\n",
       " '##oting': 20656,\n",
       " 'counters': 24094,\n",
       " 'inserted': 12889,\n",
       " 'authorised': 19256,\n",
       " 'fundraising': 15524,\n",
       " 'kilometer': 20595,\n",
       " 'analogue': 21800,\n",
       " 'veterinary': 15651,\n",
       " '##tp': 25856,\n",
       " '##ritan': 25279,\n",
       " 'appendix': 22524,\n",
       " '##rac': 22648,\n",
       " 'midday': 22878,\n",
       " 'oath': 11292,\n",
       " 'sponge': 25742,\n",
       " 'almeida': 29555,\n",
       " 'converted': 4991,\n",
       " 'doping': 23799,\n",
       " 'thinking': 3241,\n",
       " 'question': 3160,\n",
       " 'measured': 7594,\n",
       " 'sprung': 22057,\n",
       " 'terminals': 17703,\n",
       " 'charleston': 10907,\n",
       " 'wright': 6119,\n",
       " 'iowa': 5947,\n",
       " 'tribes': 6946,\n",
       " '##pone': 29513,\n",
       " '##igan': 10762,\n",
       " 'deception': 17575,\n",
       " '##chuk': 26516,\n",
       " 'il': 6335,\n",
       " 'staff': 3095,\n",
       " 'purchase': 5309,\n",
       " 'fc': 4429,\n",
       " 'calais': 23116,\n",
       " 'bribes': 29117,\n",
       " 'sandwiches': 22094,\n",
       " 'doctorate': 8972,\n",
       " 'utilize': 16462,\n",
       " 'narrator': 11185,\n",
       " 'noticing': 15103,\n",
       " 'santo': 11685,\n",
       " 'darren': 12270,\n",
       " 'rats': 11432,\n",
       " 'Õº': 1233,\n",
       " '##meral': 28990,\n",
       " '[unused865]': 870,\n",
       " 'ir': 20868,\n",
       " 'matthew': 5487,\n",
       " 'à®¿': 1396,\n",
       " '##pu': 14289,\n",
       " 'persia': 16667,\n",
       " '199': 20713,\n",
       " 'walled': 17692,\n",
       " 'constitution': 4552,\n",
       " 'wiping': 14612,\n",
       " '##yang': 12198,\n",
       " 'cocaine': 16034,\n",
       " 'sadly': 13718,\n",
       " 'midsummer': 28171,\n",
       " 'hydraulic': 14761,\n",
       " 'erebidae': 25875,\n",
       " 'dilapidated': 29283,\n",
       " 'knocking': 10591,\n",
       " 'increase': 3623,\n",
       " '##kovich': 28195,\n",
       " 'toronto': 4361,\n",
       " 'norwood': 22804,\n",
       " 'decisive': 13079,\n",
       " 'asian': 4004,\n",
       " 'than': 2084,\n",
       " 'roy': 6060,\n",
       " 'gorge': 14980,\n",
       " '##ous': 3560,\n",
       " '##rvis': 29074,\n",
       " 'universal': 5415,\n",
       " 'commands': 10954,\n",
       " 'robust': 15873,\n",
       " 'blankets': 15019,\n",
       " '[unused507]': 512,\n",
       " 'squadrons': 11435,\n",
       " 'disciplines': 12736,\n",
       " 'tehsil': 20751,\n",
       " 'enjoyed': 5632,\n",
       " '##hua': 14691,\n",
       " 'hostility': 18258,\n",
       " 'growls': 27825,\n",
       " 'reduce': 5547,\n",
       " 'werewolves': 18306,\n",
       " 'recommendations': 11433,\n",
       " 'bwf': 21215,\n",
       " '##ston': 7106,\n",
       " '1806': 12518,\n",
       " 'teamed': 12597,\n",
       " 'secrecy': 20259,\n",
       " 'completely': 3294,\n",
       " 'gregorian': 25847,\n",
       " '1924': 4814,\n",
       " '##itical': 26116,\n",
       " 'outlawed': 29131,\n",
       " 'guo': 22720,\n",
       " 'overwhelmed': 13394,\n",
       " '##ophone': 25232,\n",
       " 'manipulated': 20063,\n",
       " 'stirred': 13551,\n",
       " 'twins': 8178,\n",
       " '1865': 6725,\n",
       " 'maple': 11035,\n",
       " 'privatization': 23966,\n",
       " 'loyalist': 23414,\n",
       " 'jet': 6892,\n",
       " 'pretty': 3492,\n",
       " 'agile': 29003,\n",
       " 'grill': 18651,\n",
       " 'wisdom': 9866,\n",
       " 'truncated': 25449,\n",
       " 'latvia': 12429,\n",
       " 'clint': 16235,\n",
       " '##ains': 28247,\n",
       " 'pussy': 22418,\n",
       " '164': 17943,\n",
       " 'affiliated': 6989,\n",
       " 'baseball': 3598,\n",
       " 'refining': 28596,\n",
       " 'kg': 4705,\n",
       " 'dragon': 5202,\n",
       " 'dixie': 18910,\n",
       " 'psychiatric': 13691,\n",
       " 'mansfield': 15352,\n",
       " '##fish': 7529,\n",
       " 'minerva': 27383,\n",
       " 'comprise': 15821,\n",
       " 'lucas': 6326,\n",
       " '##kow': 24144,\n",
       " '##à§€': 29916,\n",
       " 'uncommon': 13191,\n",
       " 'colin': 6972,\n",
       " 'grandchildren': 13628,\n",
       " 'dome': 8514,\n",
       " '##hiff': 25798,\n",
       " '[unused195]': 200,\n",
       " '##dran': 24914,\n",
       " 'requirements': 5918,\n",
       " '##å³¶': 30359,\n",
       " '##de': 3207,\n",
       " 'willoughby': 24919,\n",
       " 'commodore': 12957,\n",
       " 'follow': 3582,\n",
       " 'moth': 5820,\n",
       " '##sio': 20763,\n",
       " 'lila': 19286,\n",
       " 'der': 4315,\n",
       " 'taxonomic': 27691,\n",
       " 'trait': 18275,\n",
       " 'disabilities': 13597,\n",
       " 'sniffing': 27646,\n",
       " 'breaststroke': 28164,\n",
       " 'entering': 5738,\n",
       " 'peterborough': 17587,\n",
       " 'jaipur': 28355,\n",
       " 'contains': 3397,\n",
       " '##lei': 23057,\n",
       " 'plant': 3269,\n",
       " 'shown': 3491,\n",
       " 'expect': 5987,\n",
       " 'prevents': 16263,\n",
       " 'explanation': 7526,\n",
       " 'messed': 18358,\n",
       " 'chen': 8802,\n",
       " 'conquest': 9187,\n",
       " 'natural': 3019,\n",
       " '[unused705]': 710,\n",
       " '##à¸²': 29958,\n",
       " 'initially': 3322,\n",
       " 'uncomfortably': 22502,\n",
       " 'monique': 26194,\n",
       " 'provincial': 4992,\n",
       " 'faber': 21720,\n",
       " 'fork': 9292,\n",
       " '64': 4185,\n",
       " 'dots': 14981,\n",
       " 'judy': 12120,\n",
       " 'replaces': 20736,\n",
       " 'guerrero': 16938,\n",
       " 'nm': 13221,\n",
       " 'literary': 4706,\n",
       " '##eyer': 20211,\n",
       " 'tensed': 15225,\n",
       " 'precisely': 10785,\n",
       " 'turbine': 14027,\n",
       " '##win': 10105,\n",
       " 'buckle': 22853,\n",
       " 'appointment': 6098,\n",
       " '##yle': 12844,\n",
       " 'using': 2478,\n",
       " '[unused457]': 462,\n",
       " 'conventions': 12472,\n",
       " 'fiancee': 19455,\n",
       " 'enjoy': 5959,\n",
       " 'hideous': 22293,\n",
       " 'reservoirs': 22535,\n",
       " 'widened': 8723,\n",
       " 'morphology': 19476,\n",
       " '##khand': 25910,\n",
       " 'augmented': 19335,\n",
       " 'Â¬': 1078,\n",
       " 'brawl': 23244,\n",
       " 'excavated': 15199,\n",
       " 'import': 12324,\n",
       " 'immersed': 26275,\n",
       " 'prasad': 17476,\n",
       " 'consisted': 5031,\n",
       " 'presley': 17229,\n",
       " 'menu': 12183,\n",
       " 'ãƒ’': 1719,\n",
       " 'milano': 21613,\n",
       " 'armenia': 10110,\n",
       " 'textiles': 18762,\n",
       " 'brook': 9566,\n",
       " 'bourbon': 15477,\n",
       " 'experimenting': 23781,\n",
       " '##amy': 24079,\n",
       " 'proteins': 8171,\n",
       " '##gn': 16206,\n",
       " 'parted': 10277,\n",
       " 'piers': 16067,\n",
       " 'pitchers': 23232,\n",
       " 'ribs': 10335,\n",
       " 'coated': 15026,\n",
       " '##enko': 17868,\n",
       " '##é¢': 30502,\n",
       " 'sunny': 11559,\n",
       " 'beatrice': 14807,\n",
       " 'exceeds': 23651,\n",
       " 'hayden': 13872,\n",
       " '##rane': 18053,\n",
       " 'sister': 2905,\n",
       " 'km': 2463,\n",
       " 'runaway': 19050,\n",
       " 'chemicals': 12141,\n",
       " 'nervously': 12531,\n",
       " '##iki': 17471,\n",
       " '##rization': 26910,\n",
       " 'alaska': 7397,\n",
       " 'bumps': 18548,\n",
       " 'stood': 2768,\n",
       " 'chuckle': 15375,\n",
       " 'fee': 7408,\n",
       " 'creepy': 17109,\n",
       " 'transcript': 24051,\n",
       " 'outlined': 14801,\n",
       " 'schultz': 22378,\n",
       " '##umi': 12717,\n",
       " '##onic': 12356,\n",
       " 'legged': 15817,\n",
       " 'ain': 7110,\n",
       " 'utah': 6646,\n",
       " 'belief': 6772,\n",
       " 'kawasaki': 27324,\n",
       " '##rix': 17682,\n",
       " 'iec': 24940,\n",
       " 'emphasized': 13155,\n",
       " 'impressed': 7622,\n",
       " 'chrome': 18546,\n",
       " 'acts': 4490,\n",
       " 'climb': 7105,\n",
       " 'stilled': 22791,\n",
       " 'compelled': 15055,\n",
       " 'deadline': 15117,\n",
       " 'à¦ª': 1367,\n",
       " '##hel': 16001,\n",
       " 'alley': 8975,\n",
       " 'mystery': 6547,\n",
       " 'replied': 3880,\n",
       " 'cardinals': 9310,\n",
       " '##à¯': 29933,\n",
       " 'collective': 7268,\n",
       " 'commentaries': 21241,\n",
       " '##mata': 21022,\n",
       " 'injustice': 21321,\n",
       " 'kid': 4845,\n",
       " 'campaign': 3049,\n",
       " 'scully': 25686,\n",
       " '37th': 23027,\n",
       " '##era': 6906,\n",
       " '##ault': 23505,\n",
       " 'warwick': 13283,\n",
       " '##rka': 22379,\n",
       " '##zed': 5422,\n",
       " 'added': 2794,\n",
       " 'unlikely': 9832,\n",
       " 'walks': 7365,\n",
       " 'celebrated': 6334,\n",
       " 'corporations': 11578,\n",
       " 'obstacle': 18355,\n",
       " 'divorce': 8179,\n",
       " 'accepted': 3970,\n",
       " 'save': 3828,\n",
       " '##rosis': 29166,\n",
       " 'battlefield': 11686,\n",
       " 'cheat': 21910,\n",
       " 'forecast': 19939,\n",
       " '##roids': 29514,\n",
       " 'losses': 6409,\n",
       " 'stacks': 20829,\n",
       " 'spence': 22186,\n",
       " 'denotes': 14796,\n",
       " 'regiments': 10435,\n",
       " '##main': 24238,\n",
       " 'feral': 18993,\n",
       " 'à²°': 1401,\n",
       " 'cbs': 6568,\n",
       " 'm3': 29061,\n",
       " '[unused508]': 513,\n",
       " 'exactly': 3599,\n",
       " 'prof': 11268,\n",
       " 'qui': 21864,\n",
       " '##rica': 14735,\n",
       " 'purchasing': 13131,\n",
       " '##ently': 28198,\n",
       " '##Õ¾': 29782,\n",
       " 'lays': 19764,\n",
       " '##ç©º': 30456,\n",
       " 'singers': 8453,\n",
       " 'leningrad': 15930,\n",
       " 'floods': 14295,\n",
       " '##hop': 18471,\n",
       " '##chenko': 25872,\n",
       " 'continuing': 5719,\n",
       " 'interested': 4699,\n",
       " '##ante': 12956,\n",
       " '[unused902]': 907,\n",
       " 'mahmud': 25886,\n",
       " 'debate': 5981,\n",
       " '[unused771]': 776,\n",
       " '##ype': 18863,\n",
       " 'invariably': 26597,\n",
       " 'ontario': 4561,\n",
       " 'letting': 5599,\n",
       " '315': 22904,\n",
       " 'root': 7117,\n",
       " 'racial': 5762,\n",
       " 'Ù€': 1290,\n",
       " 'flames': 7311,\n",
       " 'ant': 14405,\n",
       " 'É”': 1112,\n",
       " 'balkans': 19733,\n",
       " 'auspices': 20153,\n",
       " 'apologized': 17806,\n",
       " 'superb': 21688,\n",
       " 'tommy': 6838,\n",
       " 'randolph': 13031,\n",
       " 'janata': 20308,\n",
       " 'dig': 10667,\n",
       " 'route': 2799,\n",
       " 'styled': 13650,\n",
       " 'printed': 6267,\n",
       " 'introducing': 10449,\n",
       " '##hum': 28600,\n",
       " 'æ™º': 1869,\n",
       " 'cursing': 19752,\n",
       " '##æ™º': 30395,\n",
       " 'remind': 10825,\n",
       " 'remember': 3342,\n",
       " 'entertain': 20432,\n",
       " 'fulham': 21703,\n",
       " 'worn': 6247,\n",
       " 'clung': 14752,\n",
       " 'throughout': 2802,\n",
       " '##dam': 17130,\n",
       " 'madeira': 27309,\n",
       " 'dressing': 11225,\n",
       " 'parasite': 21198,\n",
       " 'multiplied': 28608,\n",
       " 'quilt': 27565,\n",
       " 'sai': 18952,\n",
       " 'tall': 4206,\n",
       " 'hai': 15030,\n",
       " 'safari': 23591,\n",
       " '##wr': 13088,\n",
       " '[unused250]': 255,\n",
       " 'emergence': 14053,\n",
       " 'pocket': 4979,\n",
       " 'freshwater': 12573,\n",
       " '##Úº': 29843,\n",
       " '[unused713]': 718,\n",
       " 'solve': 9611,\n",
       " 'radically': 25796,\n",
       " 'freedom': 4071,\n",
       " '##pha': 21890,\n",
       " 'geared': 23636,\n",
       " 'enthusiastic': 14727,\n",
       " 'raised': 2992,\n",
       " 'mcleod': 25363,\n",
       " 'shortages': 22623,\n",
       " 'contain': 5383,\n",
       " '##sboro': 25623,\n",
       " 'helicopters': 12400,\n",
       " '##els': 9050,\n",
       " 'adolf': 12500,\n",
       " 'releases': 7085,\n",
       " 'succeeds': 21645,\n",
       " 'merely': 6414,\n",
       " 'witchcraft': 21599,\n",
       " '[unused144]': 149,\n",
       " 'entity': 9178,\n",
       " '##up': 6279,\n",
       " '[unused177]': 182,\n",
       " '##nb': 27698,\n",
       " 's': 1055,\n",
       " 'troy': 9553,\n",
       " 'ata': 29533,\n",
       " 'process': 2832,\n",
       " '##ni': 3490,\n",
       " 'glint': 25263,\n",
       " 'c1': 27723,\n",
       " 'departure': 6712,\n",
       " 'recommended': 6749,\n",
       " '[unused586]': 591,\n",
       " 'constellation': 15300,\n",
       " 'photography': 5855,\n",
       " 'logging': 15899,\n",
       " 'covenant': 16077,\n",
       " 'feldman': 26908,\n",
       " 'saints': 6586,\n",
       " 'minus': 15718,\n",
       " 'puppy': 17022,\n",
       " 'newman': 10625,\n",
       " 'billing': 25640,\n",
       " 'award': 2400,\n",
       " 'poked': 16826,\n",
       " '##ins': 7076,\n",
       " 'managing': 6605,\n",
       " '##rst': 12096,\n",
       " '##lston': 21540,\n",
       " 'resign': 12897,\n",
       " 'beetles': 14538,\n",
       " '##200': 28332,\n",
       " 'immunity': 15403,\n",
       " '##christ': 26654,\n",
       " 'cote': 17155,\n",
       " 'wake': 5256,\n",
       " '##na': 2532,\n",
       " '##nad': 25389,\n",
       " '[unused650]': 655,\n",
       " 'pri': 26927,\n",
       " 'reasonably': 16286,\n",
       " '[unused14]': 15,\n",
       " '410': 19151,\n",
       " 'packs': 15173,\n",
       " 'waived': 16301,\n",
       " 'Â¹â„â‚‚': 18728,\n",
       " 'herd': 14906,\n",
       " 'amc': 21962,\n",
       " '##rao': 25667,\n",
       " 'tons': 6197,\n",
       " '##bilis': 27965,\n",
       " 'force': 2486,\n",
       " 'validity': 16406,\n",
       " 'starts': 4627,\n",
       " 'session': 5219,\n",
       " '##rana': 16737,\n",
       " 'seriousness': 27994,\n",
       " '##ico': 11261,\n",
       " '##nn': 10695,\n",
       " '##gnon': 26977,\n",
       " 'serial': 7642,\n",
       " '##akes': 20060,\n",
       " 'urgently': 25478,\n",
       " 'frequently': 4703,\n",
       " 'defensive': 5600,\n",
       " '##jected': 24455,\n",
       " 'lisbon': 11929,\n",
       " 'smoothly': 15299,\n",
       " ...}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.vocab#token:id\n",
    "#å°¾ç›˜åŒ¹é…å°±æ˜¯unkonwn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fff02191",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30522"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fa28c834",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'unk_token': '[UNK]',\n",
       " 'sep_token': '[SEP]',\n",
       " 'pad_token': '[PAD]',\n",
       " 'cls_token': '[CLS]',\n",
       " 'mask_token': '[MASK]'}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.special_tokens_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2650f165",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[100, 102, 0, 101, 103]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_tokens_to_ids(x for x in tokenizer.special_tokens_map.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ql7f2nh0idq",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ“– AIè¡¥å……è¯´æ˜ï¼šä»£ç ä¸­å‡ºç°çš„æ ¸å¿ƒæ¦‚å¿µè¯¦è§£\n",
    "\n",
    "### 1. **token_type_ids**ï¼ˆç‰‡æ®µç±»å‹æ ‡è¯†ç¬¦ï¼‰\n",
    "**ä½œç”¨**ï¼šåŒºåˆ†è¾“å…¥ä¸­çš„ä¸åŒå¥å­/ç‰‡æ®µï¼Œä¸»è¦ç”¨äº**å¥å­å¯¹ä»»åŠ¡**ï¼ˆSentence Pair Tasksï¼‰\n",
    "\n",
    "**å·¥ä½œåŸç†**ï¼š\n",
    "- `0`ï¼šè¡¨ç¤ºç¬¬ä¸€ä¸ªå¥å­/ç‰‡æ®µ\n",
    "- `1`ï¼šè¡¨ç¤ºç¬¬äºŒä¸ªå¥å­/ç‰‡æ®µ\n",
    "\n",
    "**åº”ç”¨åœºæ™¯**ï¼š\n",
    "- **é—®ç­”ç³»ç»Ÿ**ï¼šåŒºåˆ†é—®é¢˜(0)å’Œç­”æ¡ˆ(1)\n",
    "- **è‡ªç„¶è¯­è¨€æ¨ç†**ï¼šåŒºåˆ†å‰æ(0)å’Œå‡è®¾(1)  \n",
    "- **å¥å­ç›¸ä¼¼åº¦**ï¼šåŒºåˆ†ä¸¤ä¸ªå¾…æ¯”è¾ƒçš„å¥å­\n",
    "\n",
    "**æœ¬ä¾‹ä¸­çš„æƒ…å†µ**ï¼š\n",
    "```python\n",
    "'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0],  # å…¨ä¸º0ï¼Œå› ä¸ºæ˜¯å•å¥åˆ†ç±»ä»»åŠ¡\n",
    "                         [0, 0, 0, 0, 0, 0, 0]])\n",
    "```\n",
    "ç”±äºæƒ…æ„Ÿåˆ†ææ˜¯**å•å¥åˆ†ç±»ä»»åŠ¡**ï¼Œæ‰€ä»¥æ‰€æœ‰tokençš„typeéƒ½æ˜¯0ã€‚è¿™ä¸ªå­—æ®µåœ¨å•å¥ä»»åŠ¡ä¸­å¯ä»¥å¿½ç•¥ï¼Œä½†æ¨¡å‹ä»ä¼šè¿”å›å®ƒã€‚\n",
    "\n",
    "---\n",
    "\n",
    "### 2. **attention_mask**ï¼ˆæ³¨æ„åŠ›æ©ç ï¼‰\n",
    "**ä½œç”¨**ï¼šå‘Šè¯‰æ¨¡å‹å“ªäº›tokenæ˜¯çœŸå®çš„ï¼Œå“ªäº›æ˜¯paddingå¡«å……çš„\n",
    "\n",
    "**æ•°å€¼å«ä¹‰**ï¼š\n",
    "- `1`ï¼šçœŸå®tokenï¼Œéœ€è¦æ¨¡å‹å…³æ³¨\n",
    "- `0`ï¼špadding tokenï¼Œåº”è¯¥è¢«å¿½ç•¥\n",
    "\n",
    "**ç¤ºä¾‹**ï¼š\n",
    "```python\n",
    "'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1],  # ç¬¬ä¸€å¥æ— padding\n",
    "                         [1, 1, 1, 1, 1, 1, 0]]) # ç¬¬äºŒå¥æœ«å°¾æœ‰padding(0)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 3. **truncation ä¸ padding çš„ç»„åˆä½¿ç”¨**\n",
    "\n",
    "| å‚æ•°ç»„åˆ | æ•ˆæœ | é€‚ç”¨åœºæ™¯ |\n",
    "|---------|------|---------|\n",
    "| `padding=True` | å¡«å……åˆ°batchå†…æœ€é•¿å¥å­é•¿åº¦ | å¥å­é•¿åº¦å·®å¼‚ä¸å¤§ |\n",
    "| `padding='max_length'` | å¡«å……åˆ°æ¨¡å‹æœ€å¤§é•¿åº¦(é€šå¸¸512) | éœ€è¦å›ºå®šé•¿åº¦è¾“å…¥ |\n",
    "| `truncation=True` | è¶…è¿‡æœ€å¤§é•¿åº¦æ—¶æˆªæ–­ | å¤„ç†é•¿æ–‡æœ¬ |\n",
    "| `max_length=32` | è‡ªå®šä¹‰æœ€å¤§é•¿åº¦ | ç‰¹å®šé•¿åº¦è¦æ±‚ |\n",
    "\n",
    "**æ¨èåšæ³•**ï¼š\n",
    "```python\n",
    "# æ–¹æ¡ˆ1ï¼šåŠ¨æ€é•¿åº¦ï¼ˆèŠ‚çœå†…å­˜ï¼‰\n",
    "tokenizer(texts, padding=True, truncation=True, return_tensors='pt')\n",
    "\n",
    "# æ–¹æ¡ˆ2ï¼šå›ºå®šé•¿åº¦ï¼ˆé€‚åˆæ‰¹å¤„ç†ï¼‰\n",
    "tokenizer(texts, max_length=128, padding='max_length', \n",
    "          truncation=True, return_tensors='pt')\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 4. **æ¨¡å‹è¾“å‡ºçš„ä¸‰ä¸ªå…³é”®å±æ€§**\n",
    "\n",
    "```python\n",
    "SequenceClassifierOutput(\n",
    "    loss=None,              # è®­ç»ƒæ—¶æ‰æœ‰ï¼Œæ¨ç†æ—¶ä¸ºNone\n",
    "    logits=tensor(...),     # æœªå½’ä¸€åŒ–çš„é¢„æµ‹åˆ†æ•°\n",
    "    hidden_states=None,     # éšè—å±‚çŠ¶æ€ï¼ˆéœ€é…ç½®è¾“å‡ºï¼‰\n",
    "    attentions=None         # æ³¨æ„åŠ›æƒé‡ï¼ˆéœ€é…ç½®è¾“å‡ºï¼‰\n",
    ")\n",
    "```\n",
    "\n",
    "**logits**ï¼š\n",
    "- ç»è¿‡ç¥ç»ç½‘ç»œæœ€åä¸€å±‚ä½†æœªç»è¿‡softmaxçš„åŸå§‹è¾“å‡º\n",
    "- å€¼è¶Šå¤§è¡¨ç¤ºæ¨¡å‹è¶Šå€¾å‘äºè¯¥ç±»åˆ«\n",
    "- éœ€è¦é€šè¿‡softmaxè½¬æ¢ä¸ºæ¦‚ç‡\n",
    "\n",
    "---\n",
    "\n",
    "### 5. **ID2Label æ˜ å°„**\n",
    "```python\n",
    "\"id2label\": {\n",
    "    \"0\": \"NEGATIVE\",\n",
    "    \"1\": \"POSITIVE\"\n",
    "}\n",
    "```\n",
    "- å°†æ¨¡å‹è¾“å‡ºçš„æ•°å­—IDè½¬æ¢ä¸ºå¯è¯»çš„æ ‡ç­¾\n",
    "- `label2id` åˆ™æ˜¯åå‘æ˜ å°„\n",
    "\n",
    "---\n",
    "\n",
    "### 6. **ç‰¹æ®ŠTokençš„IDå¯¹ç…§è¡¨**\n",
    "\n",
    "| Token | å«ä¹‰ | ID | ç”¨é€” |\n",
    "|-------|------|-----|------|\n",
    "| `[UNK]` | Unknown | 100 | è¯æ±‡è¡¨ä¸­ä¸å­˜åœ¨çš„è¯ |\n",
    "| `[SEP]` | Separator | 102 | å¥å­åˆ†éš”ç¬¦ |\n",
    "| `[PAD]` | Padding | 0 | å¡«å……ç¬¦ |\n",
    "| `[CLS]` | Classification | 101 | å¥å­èµ·å§‹ç¬¦ï¼ˆç”¨äºåˆ†ç±»ï¼‰ |\n",
    "| `[MASK]` | Mask | 103 | æ©ç ç¬¦ï¼ˆç”¨äºé¢„è®­ç»ƒï¼‰ |\n",
    "\n",
    "---\n",
    "\n",
    "> **ğŸ’¡ æç¤º**ï¼šä»¥ä¸Šå†…å®¹ç”±AIæ ¹æ®Hugging Face Transformersåº“æœ€æ–°æ–‡æ¡£è¡¥å……ï¼Œå¸®åŠ©ç†è§£2022å¹´è§†é¢‘æœªæ¶µç›–çš„æ–°ç‰ˆæœ¬ç‰¹æ€§ã€‚\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7477f482",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer(test_senteces, truncation=True, padding=True, return_tensors='pt')#æŒ‡å®šæœ€å¤§é•¿åº¦ï¼Œè¶…é•¿æˆªæ–­ï¼Œè¡¥é½, padding = true äºmax_lengthä¸å…¼å®¹ï¼Œå½“æŒ‡å®špadding = Trueæ—¶ï¼Œéå†æ‰€æœ‰å¥å­ï¼Œä»¥æ‰€æœ‰å¥å­çš„æœ€é•¿é•¿åº¦ä¸ºpaddingçš„å°ºå¯¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d1dd4415",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[ 101, 2651, 2003, 2025, 2008, 2919,  102,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0],\n",
       "        [ 101, 2651, 2003, 2061, 2919,  102,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0]])}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(test_senteces, max_length=32, truncation=True,\n",
    "          padding='max_length', return_tensors='pt')  # æŒ‡å®šæœ€å¤§é•¿åº¦ï¼Œè¶…é•¿æˆªæ–­ï¼Œè¡¥é½"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e280048",
   "metadata": {},
   "source": [
    "# 2.modeï¼Œè°ƒç”¨æ¨¡å‹ & parse output ï¼Œè¾“å‡ºè§£æ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "89934ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4b2dbd6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DistilBertConfig {\n",
       "  \"activation\": \"gelu\",\n",
       "  \"architectures\": [\n",
       "    \"DistilBertForSequenceClassification\"\n",
       "  ],\n",
       "  \"attention_dropout\": 0.1,\n",
       "  \"bos_token_id\": null,\n",
       "  \"dim\": 768,\n",
       "  \"dropout\": 0.1,\n",
       "  \"dtype\": \"float32\",\n",
       "  \"eos_token_id\": null,\n",
       "  \"finetuning_task\": \"sst-2\",\n",
       "  \"hidden_dim\": 3072,\n",
       "  \"id2label\": {\n",
       "    \"0\": \"NEGATIVE\",\n",
       "    \"1\": \"POSITIVE\"\n",
       "  },\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"label2id\": {\n",
       "    \"NEGATIVE\": 0,\n",
       "    \"POSITIVE\": 1\n",
       "  },\n",
       "  \"max_position_embeddings\": 512,\n",
       "  \"model_type\": \"distilbert\",\n",
       "  \"n_heads\": 12,\n",
       "  \"n_layers\": 6,\n",
       "  \"output_past\": true,\n",
       "  \"pad_token_id\": 0,\n",
       "  \"qa_dropout\": 0.1,\n",
       "  \"seq_classif_dropout\": 0.2,\n",
       "  \"sinusoidal_pos_embds\": false,\n",
       "  \"tie_weights_\": true,\n",
       "  \"tie_word_embeddings\": true,\n",
       "  \"transformers_version\": \"5.1.0\",\n",
       "  \"vocab_size\": 30522\n",
       "}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "cfd4da61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SequenceClassifierOutput(loss=None, logits=tensor([[-3.4620,  3.6118],\n",
      "        [ 4.7508, -3.7899]]), hidden_states=None, attentions=None)\n",
      "tensor([[8.4631e-04, 9.9915e-01],\n",
      "        [9.9980e-01, 1.9531e-04]])\n",
      "tensor([1, 0])\n",
      "['POSITIVE', 'NEGATIVE']\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    outputs = model(**batch_input)\n",
    "    print(outputs)\n",
    "    score = F.softmax(outputs.logits, dim=1)\n",
    "    print(score)\n",
    "    labels = torch.argmax(score,dim=1)\n",
    "    print(labels)\n",
    "    labels = [model.config.id2label[id] for id in labels.tolist()]\n",
    "    print(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "618fac6e",
   "metadata": {},
   "source": [
    "ç»è¿‡ä¸€ç³»åˆ—ç¥ç»ç½‘ç»œï¼Œé€åˆ°softmax ä¹‹å‰çš„ç»“æœä¸€èˆ¬æ¥è¯´å«åšlogits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb466035",
   "metadata": {},
   "source": [
    "é»˜è®¤æ¨¡å‹\n",
    "In [1]: from transformers import pipeline\n",
    "In [2]: task_name = 'sentiment-analysis'\n",
    "\n",
    "In [3]: pipeline(task_name)\n",
    "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision 714eb0f.\n",
    "Using a pipeline without specifying a model name and revision in production is not recommended."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "xyerdxp50f",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸš€ Transformersåº“æœ€æ–°ç‰ˆæœ¬ï¼ˆv5.xï¼‰çš„é‡è¦å˜åŒ–\n",
    "\n",
    "### ä¸2022å¹´ç‰ˆæœ¬çš„ä¸»è¦å·®å¼‚\n",
    "\n",
    "#### 1. **é»˜è®¤æ¨¡å‹å˜æ›´**\n",
    "```python\n",
    "# æ—§ç‰ˆæœ¬ï¼ˆ2022ï¼‰\n",
    "task_name = 'sentiment-analysis'\n",
    "pipeline(task_name)  \n",
    "# é»˜è®¤: distilbert-base-uncased-ffinetuned-sst-2-english\n",
    "\n",
    "# æ–°ç‰ˆæœ¬ï¼ˆ2024+ï¼‰\n",
    "pipeline(task_name)\n",
    "# é»˜è®¤: distilbert/distilbert-base-uncased-ffinetuned-sst-2-english\n",
    "# æ³¨æ„ï¼šæ¨¡å‹è·¯å¾„å¢åŠ äº†ç»„ç»‡åç§°å‰ç¼€\n",
    "```\n",
    "\n",
    "#### 2. **safe_tensors æ ¼å¼æˆä¸ºæ ‡å‡†**\n",
    "```python\n",
    "# æ—§ç‰ˆæœ¬ï¼šæ¨¡å‹æ–‡ä»¶ä¸º .bin (PyTorchæ ¼å¼)\n",
    "# model.bin\n",
    "\n",
    "# æ–°ç‰ˆæœ¬ï¼šé»˜è®¤ä½¿ç”¨ .safetensors (æ›´å®‰å…¨ã€æ›´å¿«çš„åŠ è½½æ ¼å¼)\n",
    "# model.safetensors\n",
    "```\n",
    "\n",
    "**ä¼˜åŠ¿**ï¼š\n",
    "- âœ… å®‰å…¨æ€§ï¼šé˜²æ­¢æ¶æ„æ¨¡å‹æ–‡ä»¶\n",
    "- âœ… é€Ÿåº¦ï¼šåŠ è½½é€Ÿåº¦æ›´å¿«\n",
    "- âœ… è·¨å¹³å°ï¼šæ›´å¥½çš„å…¼å®¹æ€§\n",
    "\n",
    "#### 3. **æ–°æ¨èçš„æœ€ä½³å®è·µ**\n",
    "\n",
    "**A. ä½¿ç”¨Auto Classesæ—¶æŒ‡å®šæ¨¡å‹ä¿®è®¢ç‰ˆæœ¬**ï¼š\n",
    "```python\n",
    "# æ¨èï¼šæŒ‡å®šrevisionä»¥ç¡®ä¿å¯å¤ç°æ€§\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"distilbert/distilbert-base-uncased-finetuned-sst-2-english\",\n",
    "    revision=\"714eb0f\"  # å›ºå®šç‰¹å®šç‰ˆæœ¬\n",
    ")\n",
    "\n",
    "# ä¸æ¨èï¼šæ¯æ¬¡å¯èƒ½åŠ è½½ä¸åŒç‰ˆæœ¬\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"distilbert/distilbert-base-uncased-finetuned-sst-2-english\"\n",
    ")\n",
    "```\n",
    "\n",
    "**B. è®¾ç½®ç¯å¢ƒå˜é‡é¿å…è­¦å‘Š**ï¼š\n",
    "```python\n",
    "import os\n",
    "os.environ['HF_HUB_DISABLE_SYMLINKS_WARNING'] = '1'\n",
    "```\n",
    "\n",
    "**C. ä½¿ç”¨tokenizersçš„æ›´å¿«å®ç°**ï¼š\n",
    "```python\n",
    "# æ–°ç‰ˆæœ¬é»˜è®¤ä½¿ç”¨Rustå®ç°çš„tokenizerï¼ˆé€Ÿåº¦å¿«10-100å€ï¼‰\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "# æ— éœ€é¢å¤–é…ç½®ï¼Œè‡ªåŠ¨ä½¿ç”¨fast tokenizer\n",
    "```\n",
    "\n",
    "#### 4. **è¾“å‡ºå¯¹è±¡çš„æ–°æ–¹æ³•**\n",
    "\n",
    "```python\n",
    "outputs = model(**batch_input)\n",
    "\n",
    "# æ–°å¢æ–¹æ³•ï¼šç›´æ¥è·å–é¢„æµ‹ç»“æœ\n",
    "predictions = outputs.logits.argmax(dim=-1)\n",
    "\n",
    "# æ–°å¢ï¼šä½¿ç”¨åå¤„ç†å‡½æ•°æ›´æ–¹ä¾¿\n",
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline(\"sentiment-analysis\", model=model_name)\n",
    "results = classifier(test_senteces)\n",
    "# è¾“å‡ºï¼š[{'label': 'POSITIVE', 'score': 0.9991}, \n",
    "#        {'label': 'NEGATIVE', 'score': 0.9998}]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### ä»£ç ç°ä»£åŒ–å»ºè®®\n",
    "\n",
    "#### âŒ æ—§å†™æ³•\n",
    "```python\n",
    "import torch\n",
    "with torch.no_grad():  # æ‰‹åŠ¨ç®¡ç†æ¢¯åº¦\n",
    "    outputs = model(**inputs)\n",
    "    logits = outputs.logits\n",
    "    predictions = torch.argmax(logits, dim=1)\n",
    "```\n",
    "\n",
    "#### âœ… æ–°å†™æ³•ï¼ˆæ¨èï¼‰\n",
    "```python\n",
    "# æ–¹æ³•1ï¼šä½¿ç”¨pipelineï¼ˆæœ€ç®€å•ï¼‰\n",
    "from transformers import pipeline\n",
    "classifier = pipeline(\"sentiment-analysis\", model=model_name)\n",
    "results = classifier(test_senteces)\n",
    "\n",
    "# æ–¹æ³•2ï¼šä½¿ç”¨trainerï¼ˆè®­ç»ƒæ—¶ï¼‰\n",
    "from transformers import Trainer\n",
    "trainer = Trainer(model=model)\n",
    "predictions = trainer.predict(test_dataset)\n",
    "\n",
    "# æ–¹æ³•3ï¼šä½¿ç”¨inferenceæ¨¡å¼ï¼ˆæ¨ç†æ—¶ï¼‰\n",
    "model.eval()  # è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼\n",
    "with torch.inference_mode():  # æ¯”no_grad()æ›´å¿«\n",
    "    outputs = model(**inputs)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### æ€§èƒ½ä¼˜åŒ–æç¤º\n",
    "\n",
    "```python\n",
    "# 1. å¯ç”¨ç¼“å­˜ï¼ˆåŠ é€Ÿé‡å¤è¾“å…¥ï¼‰\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# 2. æ‰¹å¤„ç†ï¼ˆæ¯”å•ä¸ªå¤„ç†å¿«å¾—å¤šï¼‰\n",
    "texts = [\"text1\", \"text2\", \"text3\", ...]  # ä¸€æ¬¡å¤„ç†å¤šä¸ª\n",
    "inputs = tokenizer(texts, padding=True, return_tensors='pt')\n",
    "\n",
    "# 3. ä½¿ç”¨åŠç²¾åº¦ï¼ˆåœ¨æ”¯æŒGPUæ—¶ï¼‰\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name, \n",
    "    torch_dtype=torch.float16  # ä½¿ç”¨FP16åŠ é€Ÿ\n",
    ")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "> **ğŸ“ ç‰ˆæœ¬ä¿¡æ¯**ï¼š\n",
    "> - è§†é¢‘å½•åˆ¶æ—¶é—´ï¼š2022å¹´\n",
    "> - å½“å‰Transformersç‰ˆæœ¬ï¼š5.1.0+\n",
    "> - ä¸»è¦å˜åŒ–ï¼šæ¨¡å‹å‘½åè§„èŒƒã€safetensorsæ ¼å¼ã€æ€§èƒ½ä¼˜åŒ–\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "x2ykkq2voph",
   "source": "---\n\n## ğŸ“š Transformers Pipeline æ ‡å‡†ä»»åŠ¡åç§°å¤§å…¨\n\n### ä»€ä¹ˆæ˜¯ Pipeline ä»»åŠ¡åç§°ï¼Ÿ\n\n**Pipeline ä»»åŠ¡åç§°**æ˜¯ Hugging Face Transformers åº“ä¸­å®šä¹‰çš„**æ ‡å‡†åŒ–ä»»åŠ¡æ ‡è¯†ç¬¦**ã€‚ä½¿ç”¨è¿™äº›æ ‡å‡†åç§°ï¼Œ`pipeline()` å‡½æ•°å¯ä»¥è‡ªåŠ¨ï¼š\n- åŒ¹é…å¯¹åº”çš„ Pipeline ç±»\n- åŠ è½½åŒ¹é…çš„ Tokenizerã€Modelã€Processor\n- æä¾›ç»Ÿä¸€çš„ API æ¥å£\n\n**ç¤ºä¾‹**ï¼š\n```python\nfrom transformers import pipeline\n\n# æ–¹å¼1ï¼šä»…æŒ‡å®šä»»åŠ¡åï¼ˆè‡ªåŠ¨é€‰æ‹©é»˜è®¤æ¨¡å‹ï¼‰\nclassifier = pipeline(\"sentiment-analysis\")\n# ç­‰ä»·äºï¼špipeline(model=\"distilbert/distilbert-base-uncased-finetuned-sst-2-english\")\n\n# æ–¹å¼2ï¼šæŒ‡å®šä»»åŠ¡å’Œæ¨¡å‹\nclassifier = pipeline(\"sentiment-analysis\", model=\"nlptown/bert-base-multilingual-uncased-sentiment\")\n\n# æ–¹å¼3ï¼šä»…æŒ‡å®šæ¨¡å‹ï¼ˆè‡ªåŠ¨æ¨æ–­ä»»åŠ¡ï¼‰\nclassifier = pipeline(model=\"distilbert/distilbert-base-uncased-finetuned-sst-2-english\")\n```\n\n---\n\n### ğŸ“ è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡\n\n| ä»»åŠ¡åç§° | åˆ«å/æè¿° | é»˜è®¤æ¨¡å‹ç¤ºä¾‹ |\n|---------|----------|------------|\n| **`sentiment-analysis`** | æ–‡æœ¬åˆ†ç±»ï¼ˆç‰¹æŒ‡æƒ…æ„Ÿåˆ†æï¼‰ | `distilbert/distilbert-base-uncased-finetuned-sst-2-english` |\n| **`text-classification`** | é€šç”¨æ–‡æœ¬åˆ†ç±» | åŒä¸Š |\n| **`text-generation`** | æ–‡æœ¬ç”Ÿæˆ | `openai-community/gpt2` |\n| **`ner`** | å‘½åå®ä½“è¯†åˆ« | `dbmdz/bert-large-cased-finetuned-conll03-english` |\n| **`question-answering`** | æŠ½å–å¼é—®ç­” | `deepset/roberta-base-squad2` |\n| **`fill-mask`** | æ©ç å¡«å…… | `google-bert/bert-base-uncased` |\n| **`summarization`** | æ–‡æœ¬æ‘˜è¦ | `facebook/bart-large-cnn` |\n| **`translation`** | æœºå™¨ç¿»è¯‘ | `facebook/mbart-large-50-many-to-many-mmt` |\n| **`text2text-generation`** | æ–‡æœ¬åˆ°æ–‡æœ¬ç”Ÿæˆ | `google/flan-t5-base` |\n| **`zero-shot-classification`** | é›¶æ ·æœ¬åˆ†ç±» | `facebook/bart-large-mnli` |\n| **`table-question-answering`** | è¡¨æ ¼é—®ç­” | `google/tapas-base-finetuned-wtq` |\n\n**ä½¿ç”¨ç¤ºä¾‹**ï¼š\n```python\n# æƒ…æ„Ÿåˆ†æ\nclassifier = pipeline(\"sentiment-analysis\")\nresult = classifier(\"I love this product!\")\n# è¾“å‡º: [{'label': 'POSITIVE', 'score': 0.9998}]\n\n# å‘½åå®ä½“è¯†åˆ«\nner = pipeline(\"ner\", aggregation_strategy=\"simple\")\nresult = ner(\"My name is Wolfgang and I live in Berlin\")\n# è¾“å‡º: [{'entity_group': 'PER', 'word': 'Wolfgang'}, \n#        {'entity_group': 'LOC', 'word': 'Berlin'}]\n\n# é—®ç­”\nqa = pipeline(\"question-answering\")\nresult = qa(question=\"Where do I live?\", context=\"My name is Wolfgang and I live in Berlin\")\n# è¾“å‡º: {'score': 0.91, 'start': 34, 'end': 40, 'answer': 'Berlin'}\n```\n\n---\n\n### ğŸ”Š éŸ³é¢‘ä»»åŠ¡\n\n| ä»»åŠ¡åç§° | åˆ«å/æè¿° | é»˜è®¤æ¨¡å‹ç¤ºä¾‹ |\n|---------|----------|------------|\n| **`audio-classification`** | éŸ³é¢‘åˆ†ç±» | `superb/wav2vec2-base-superb-ks` |\n| **`automatic-speech-recognition`** | è¯­éŸ³è¯†åˆ« | `openai/whisper-base` |\n| **`text-to-speech`** | æ–‡æœ¬è½¬è¯­éŸ³ | `suno/bark-small` |\n| **`text-to-audio`** | æ–‡æœ¬è½¬éŸ³é¢‘ï¼ˆåŒä¸Šï¼‰ | `suno/bark-small` |\n| **`zero-shot-audio-classification`** | é›¶æ ·æœ¬éŸ³é¢‘åˆ†ç±» | `laion/clap-htsat-unfused` |\n\n**ä½¿ç”¨ç¤ºä¾‹**ï¼š\n```python\n# è¯­éŸ³è¯†åˆ«\nasr = pipeline(\"automatic-speech-recognition\")\nresult = asr(\"https://huggingface.co/datasets/Narsil/asr_dummy/resolve/main/1.flac\")\n\n# éŸ³é¢‘åˆ†ç±»\naudio_classifier = pipeline(\"audio-classification\")\nresult = audio_classifier(\"https://example.com/audio.wav\")\n# è¾“å‡º: [{'score': 0.997, 'label': 'cat'}, ...]\n```\n\n---\n\n### ğŸ–¼ï¸ è®¡ç®—æœºè§†è§‰ä»»åŠ¡\n\n| ä»»åŠ¡åç§° | åˆ«å/æè¿° | é»˜è®¤æ¨¡å‹ç¤ºä¾‹ |\n|---------|----------|------------|\n| **`image-classification`** | å›¾åƒåˆ†ç±» | `microsoft/beit-base-patch16-224-pt22k-ft22k` |\n| **`object-detection`** | ç›®æ ‡æ£€æµ‹ | `facebook/detr-resnet-50` |\n| **`image-segmentation`** | å›¾åƒåˆ†å‰² | `facebook/detr-resnet-50-panoptic` |\n| **`depth-estimation`** | æ·±åº¦ä¼°è®¡ | `LiheYoung/depth-anything-base-hf` |\n| **`video-classification`** | è§†é¢‘åˆ†ç±» | `microsoft/swin-tiny-patch4-window7-224` |\n| **`zero-shot-image-classification`** | é›¶æ ·æœ¬å›¾åƒåˆ†ç±» | `google/siglip-so400m-patch14-384` |\n| **`zero-shot-object-detection`** | é›¶æ ·æœ¬ç›®æ ‡æ£€æµ‹ | `google/owlvit-base-patch32` |\n| **`image-to-image`** | å›¾åƒåˆ°å›¾åƒè½¬æ¢ | `caidas/swin2SR-classical-sr-x2-64` |\n| **`mask-generation`** | æ©ç ç”Ÿæˆ | `facebook/sam-vit-base` |\n\n**ä½¿ç”¨ç¤ºä¾‹**ï¼š\n```python\n# å›¾åƒåˆ†ç±»\nclassifier = pipeline(\"image-classification\")\nresult = classifier(\"https://example.com/image.jpg\")\n# è¾“å‡º: [{'score': 0.44, 'label': 'macaw'}, ...]\n\n# ç›®æ ‡æ£€æµ‹\ndetector = pipeline(\"object-detection\")\nresult = detector(\"https://example.com/image.jpg\")\n# è¾“å‡º: [{'score': 0.99, 'label': 'cat', 'box': {'xmin': 69, 'ymin': 171, ...}}]\n\n# é›¶æ ·æœ¬å›¾åƒåˆ†ç±»\nclassifier = pipeline(\"zero-shot-image-classification\")\nresult = classifier(\n    \"https://example.com/image.jpg\",\n    candidate_labels=[\"cat\", \"dog\", \"bird\"]\n)\n```\n\n---\n\n### ğŸ”€ å¤šæ¨¡æ€ä»»åŠ¡\n\n| ä»»åŠ¡åç§° | åˆ«å/æè¿° | é»˜è®¤æ¨¡å‹ç¤ºä¾‹ |\n|---------|----------|------------|\n| **`visual-question-answering`** / **`vqa`** | è§†è§‰é—®ç­” | `dandelin/vilt-b32-finetuned-vqa` |\n| **`document-question-answering`** | æ–‡æ¡£é—®ç­” | `impira/layoutlm-document-qa` |\n| **`image-text-to-text`** | å›¾åƒæ–‡æœ¬åˆ°æ–‡æœ¬ | `Salesforce/blip-image-captioning-base` |\n| **`any-to-any`** | é€šç”¨å¤šæ¨¡æ€ç”Ÿæˆ | `google/gemma-3-27b-it` |\n\n**ä½¿ç”¨ç¤ºä¾‹**ï¼š\n```python\n# è§†è§‰é—®ç­”\nvqa = pipeline(\"visual-question-answering\")\nresult = vqa(\n    image=\"https://example.com/image.jpg\",\n    question=\"What is in this image?\"\n)\n# è¾“å‡º: [{'score': 0.94, 'answer': 'a cat'}]\n\n# å›¾åƒæè¿°ç”Ÿæˆ\ncaptioner = pipeline(\"image-to-text\", model=\"Salesforce/blip-image-captioning-base\")\nresult = captioner(\"https://example.com/image.jpg\", text=\"A photo of\")\n# è¾“å‡º: [{'generated_text': 'a photo of two birds'}]\n```\n\n---\n\n### âš™ï¸ ç‰¹å¾æå–\n\n| ä»»åŠ¡åç§° | æè¿° | é€‚ç”¨æ¨¡å‹ |\n|---------|------|---------|\n| **`feature-extraction`** | æ–‡æœ¬ç‰¹å¾æå– | æ‰€æœ‰æ–‡æœ¬æ¨¡å‹ |\n| **`image-feature-extraction`** | å›¾åƒç‰¹å¾æå– | æ‰€æœ‰è§†è§‰æ¨¡å‹ |\n\n**ä½¿ç”¨ç¤ºä¾‹**ï¼š\n```python\n# æå–æ–‡æœ¬ç‰¹å¾\nextractor = pipeline(\"feature-extraction\", model=\"google-bert/bert-base-uncased\")\nfeatures = extractor(\"This is a test\")\n# è¾“å‡ºå½¢çŠ¶: torch.Size([1, 8, 768]) - 1ä¸ªæ ·æœ¬, 8ä¸ªtoken, 768ç»´ç‰¹å¾\n\n# æå–å›¾åƒç‰¹å¾\nimg_extractor = pipeline(\"image-feature-extraction\", model=\"google/vit-base-patch16-224\")\nfeatures = img_extractor(\"https://example.com/image.jpg\")\n# è¾“å‡ºå½¢çŠ¶: torch.Size([1, 197, 768])\n```\n\n---\n\n### ğŸ’¡ å®ç”¨æŠ€å·§\n\n#### 1. **æŸ¥çœ‹æ‰€æœ‰æ”¯æŒçš„ä»»åŠ¡**\n```python\nfrom transformers import pipeline, PIPELINE_REGISTRY\n\n# æŸ¥çœ‹æ‰€æœ‰æ”¯æŒçš„ä»»åŠ¡\nfor task in sorted(PIPELINE_REGISTRY.get_supported_tasks()):\n    print(task)\n```\n\n#### 2. **æŸ¥çœ‹æ¨¡å‹æ”¯æŒçš„ä»»åŠ¡**\n```python\nfrom transformers import AutoConfig\n\nconfig = AutoConfig.from_pretrained(\"distilbert/distilbert-base-uncased-finetuned-sst-2-english\")\nprint(config.id2label)  # æŸ¥çœ‹æ ‡ç­¾æ˜ å°„\n```\n\n#### 3. **ç”Ÿäº§ç¯å¢ƒæ¨èå†™æ³•**\n```python\nimport os\n\n# æ¨èï¼šæ˜ç¡®æŒ‡å®šæ¨¡å‹ã€ç‰ˆæœ¬å’Œtoken\nclassifier = pipeline(\n    task=\"sentiment-analysis\",\n    model=\"distilbert/distilbert-base-uncased-finetuned-sst-2-english\",\n    revision=\"714eb0f\",  # å›ºå®šç‰ˆæœ¬ï¼Œç¡®ä¿å¯å¤ç°\n    token=os.getenv(\"HF_TOKEN\")  # æé«˜ä¸‹è½½é€Ÿç‡\n)\n\n# è®¾ç½®ç¯å¢ƒå˜é‡é¿å…è­¦å‘Š\nos.environ['HF_HUB_DISABLE_SYMLINKS_WARNING'] = '1'\n```\n\n#### 4. **æ‰¹é‡å¤„ç†**\n```python\n# æ‰¹é‡å¤„ç†æ¯”å•ä¸ªå¤„ç†å¿«å¾—å¤š\ntexts = [\"I love this!\", \"This is bad\", \"Amazing product!\"]\nclassifier = pipeline(\"sentiment-analysis\")\nresults = classifier(texts)\n# ä¸€æ¬¡å¤„ç†å¤šä¸ªï¼Œè‡ªåŠ¨åˆ©ç”¨æ‰¹å¤„ç†åŠ é€Ÿ\n```\n\n---\n\n### ğŸ¯ ä»»åŠ¡é€‰æ‹©æŒ‡å—\n\n**å¦‚æœä½ çš„ä»»åŠ¡æ˜¯...** â†’ **ä½¿ç”¨è¿™ä¸ªä»»åŠ¡åç§°**\n\n- åˆ¤æ–­æ–‡æœ¬æƒ…æ„Ÿï¼ˆæ­£é¢/è´Ÿé¢ï¼‰ â†’ `\"sentiment-analysis\"`\n- è¯†åˆ«æ–‡æœ¬ä¸­çš„äººåã€åœ°åã€ç»„ç»‡å â†’ `\"ner\"`\n- å›ç­”åŸºäºç»™å®šæ®µè½çš„é—®é¢˜ â†’ `\"question-answering\"`\n- ç”Ÿæˆæ–‡ç« æ‘˜è¦ â†’ `\"summarization\"`\n- ç¿»è¯‘æ–‡æœ¬ â†’ `\"translation_xx_to_yy\"`\n- è¯†åˆ«å›¾ç‰‡ä¸­çš„ç‰©ä½“ â†’ `\"object-detection\"`\n- å°†å›¾ç‰‡è½¬æ¢ä¸ºæ–‡å­—æè¿° â†’ `\"image-to-text\"`\n- è¯­éŸ³è½¬æ–‡å­— â†’ `\"automatic-speech-recognition\"`\n- æ–‡å­—è½¬è¯­éŸ³ â†’ `\"text-to-speech\"`\n\n---\n\n> **ğŸ“š å‚è€ƒèµ„æ–™**ï¼š\n> - [Hugging Face Transformers Pipeline å®˜æ–¹æ–‡æ¡£](https://huggingface.co/docs/transformers/main_classes/pipelines)\n> - [Hugging Face Hub ä»»åŠ¡æ ‡ç­¾](https://huggingface.co/docs/hub/models-tasks)\n> - [Pipeline æ•™ç¨‹](https://huggingface.co/docs/transformers/pipeline_tutorial)\n\n",
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}