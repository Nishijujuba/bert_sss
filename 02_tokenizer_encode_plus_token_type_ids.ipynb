{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1e7a99c1",
   "metadata": {},
   "source": [
    " # 1.åŠ è½½æ¨¡å‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6a2b8847",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9185716d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'bert-base-uncased'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0c3b5b8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b11ce901e861417d8cd9d2d822166c51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\juju\\AppData\\Roaming\\Python\\Python312\\site-packages\\huggingface_hub\\file_download.py:130: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\juju\\.cache\\huggingface\\hub\\models--bert-base-uncased. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a57c257f6bdb429aa7085a05d8e3d429",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a4dca7769da4b0d9b6132af3a7fd526",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bd52f61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertTokenizer(name_or_path='bert-base-uncased', vocab_size=30522, model_max_length=512, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, added_tokens_decoder={\n",
       "\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t100: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t101: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t102: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t103: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer\n",
    "#vocab_size 30522: token >ids æ˜ å°„\n",
    "#æœ€å¤§é•¿åº¦512\n",
    "#å³ä¾§æ·»åŠ padding\n",
    "#ä»å³ä¾§æˆªæ–­\n",
    "#special token map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d71b3a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "special_tokens = list(tokenizer.special_tokens_map.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c0e65e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[UNK]', '[SEP]', '[PAD]', '[CLS]', '[MASK]']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "special_tokens\n",
    "#sep å¥å°¾ CLS å¥é¦–"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "91e7c1b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[100, 102, 0, 101, 103]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_tokens_to_ids(special_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "feb71dbc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[101, 100, 102],\n",
       " [101, 102, 102],\n",
       " [101, 0, 102],\n",
       " [101, 101, 102],\n",
       " [101, 103, 102]]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(special_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48ec7b01",
   "metadata": {},
   "source": [
    "# 2è®¤è¯†æ–‡æœ¬è¯­æ–™"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c962831",
   "metadata": {},
   "source": [
    "- newsgroups_train.DESCR\n",
    "- newsgroups_train.data\n",
    "- newsgroups_train.target\n",
    "- newsgroups_train.target_names "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "02b94983",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d4986f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "newsgroup_train = fetch_20newsgroups(subset='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "360e787a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(newsgroup_train.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c77352e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11314"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(newsgroup_train.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "10ce6ab7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11314"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(newsgroup_train.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e6c8ef86",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "640a3b67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({10: 600,\n",
       "         15: 599,\n",
       "         8: 598,\n",
       "         9: 597,\n",
       "         11: 595,\n",
       "         7: 594,\n",
       "         13: 594,\n",
       "         14: 593,\n",
       "         5: 593,\n",
       "         2: 591,\n",
       "         12: 591,\n",
       "         3: 590,\n",
       "         6: 585,\n",
       "         1: 584,\n",
       "         4: 578,\n",
       "         17: 564,\n",
       "         16: 546,\n",
       "         0: 480,\n",
       "         18: 465,\n",
       "         19: 377})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(newsgroup_train.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "08eda75a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['alt.atheism',\n",
       " 'comp.graphics',\n",
       " 'comp.os.ms-windows.misc',\n",
       " 'comp.sys.ibm.pc.hardware',\n",
       " 'comp.sys.mac.hardware',\n",
       " 'comp.windows.x',\n",
       " 'misc.forsale',\n",
       " 'rec.autos',\n",
       " 'rec.motorcycles',\n",
       " 'rec.sport.baseball',\n",
       " 'rec.sport.hockey',\n",
       " 'sci.crypt',\n",
       " 'sci.electronics',\n",
       " 'sci.med',\n",
       " 'sci.space',\n",
       " 'soc.religion.christian',\n",
       " 'talk.politics.guns',\n",
       " 'talk.politics.mideast',\n",
       " 'talk.politics.misc',\n",
       " 'talk.religion.misc']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newsgroup_train.target_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96cbe737",
   "metadata": {},
   "source": [
    "# 3tokenizer è¡¥å……"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c597be13",
   "metadata": {},
   "source": [
    "- input_idsï¼Œattention_mask \n",
    "    - mask:bert å¦ä¸€ä¸ªä¸è®­ç»ƒä»»åŠ¡ï¼Œ mlmï¼Œå¯¹word æ·»åŠ æ©ç ï¼Œæ·»åŠ ä¹‹åmaskå³ä¸º1\n",
    "- encode_plus å·²ç»æ¶ˆå¤±\n",
    "- token_type_ids\n",
    "    - token_typeï¼š0 > ç¬¬ä¸€å¥ã€ä¾‹å¥ï¼›1 > ç¬¬äºŒå¥ã€‚å¯ä»¥é€šè¿‡ tokenizer() (tokenizer.\\_\\_call\\_\\_ï¼šæ‰€æœ‰å¥å­éƒ½ä¸º0)æˆ–è€…é€šè¿‡tokenizer.encodeç”Ÿæˆã€è¿”å›ï¼Œå‰ä¸€å¥è¢«è®¾ç½®ä¸º0åä¸€å¥è®¾ç½®ä¸º1ã€‚\n",
    "    - å¥å­å¯¹ä¸€èˆ¬ä½¿ç”¨åœ¨ nsp ï¼ˆnext sentence predictï¼‰ä¸‹ä¸€å¥å­é¢„æµ‹ä»»åŠ¡ä¸Šï¼Œbertçš„é¢„è®­ç»ƒä»»åŠ¡\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e7be37db",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_news = newsgroup_train.data[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "159df236",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a00ce45b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[721, 858, 1981]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#len(test_news[0])\n",
    "list(map(len, test_news))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c999ea9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[101, 2013, 1024, 3393, 2099, 2595, 3367, 1030, 11333, 2213, 1012, 8529, 2094, 1012, 3968, 2226, 1006, 2073, 1005, 1055, 2026, 2518, 1007, 3395, 1024, 2054, 2482, 2003, 2023, 999, 1029, 102], [101, 2013, 1024, 3124, 5283, 2080, 1030, 9806, 1012, 1057, 1012, 2899, 1012, 3968, 2226, 1006, 3124, 13970, 2080, 1007, 3395, 1024, 9033, 5119, 8554, 1011, 2345, 2655, 12654, 1024, 2345, 102], [101, 2013, 1024, 1056, 29602, 6856, 1030, 14925, 1012, 14925, 2078, 1012, 19749, 1012, 3968, 2226, 1006, 2726, 1041, 12688, 1007, 3395, 1024, 1052, 2497, 3980, 1012, 1012, 1012, 3029, 1024, 102]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(test_news,max_length=32,truncation=True)\n",
    "#å¥å­çº§åˆ«ï¼Œä¸æŒ‡å®šçš„æƒ…å†µä¸‹ token_type_idså‡ä¸º0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d28f0c4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KeysView({'input_ids': [[101, 2013, 1024, 3393, 2099, 2595, 3367, 1030, 11333, 2213, 1012, 8529, 2094, 1012, 3968, 2226, 1006, 2073, 1005, 1055, 2026, 2518, 1007, 3395, 1024, 2054, 2482, 2003, 2023, 999, 1029, 102], [101, 2013, 1024, 3124, 5283, 2080, 1030, 9806, 1012, 1057, 1012, 2899, 1012, 3968, 2226, 1006, 3124, 13970, 2080, 1007, 3395, 1024, 9033, 5119, 8554, 1011, 2345, 2655, 12654, 1024, 2345, 102], [101, 2013, 1024, 1056, 29602, 6856, 1030, 14925, 1012, 14925, 2078, 1012, 19749, 1012, 3968, 2226, 1006, 2726, 1041, 12688, 1007, 3395, 1024, 1052, 2497, 3980, 1012, 1012, 1012, 3029, 1024, 102]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]})\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer(test_news, max_length=32,truncation=True).keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a34e9e29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;31mSignature:\u001b[0m\n",
      "\u001b[0mtokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mtext\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'TextInput | PreTokenizedInput | EncodedInput'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mtext_pair\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'TextInput | PreTokenizedInput | EncodedInput | None'\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0madd_special_tokens\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'bool'\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mpadding\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'bool | str | PaddingStrategy'\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mtruncation\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'bool | str | TruncationStrategy | None'\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mmax_length\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'int | None'\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mstride\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'int'\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mpadding_side\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'str | None'\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mreturn_tensors\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'str | TensorType | None'\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;34m'list[int]'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mDocstring:\u001b[0m\n",
      "Converts a string to a sequence of ids (integer), using the tokenizer and vocabulary.\n",
      "\n",
      "Same as doing `self.convert_tokens_to_ids(self.tokenize(text))`.\n",
      "\n",
      "Args:\n",
      "    text (`str`, `list[str]` or `list[int]`):\n",
      "        The first sequence to be encoded. This can be a string, a list of strings (tokenized string using the\n",
      "        `tokenize` method) or a list of integers (tokenized string ids using the `convert_tokens_to_ids`\n",
      "        method).\n",
      "    text_pair (`str`, `list[str]` or `list[int]`, *optional*):\n",
      "        Optional second sequence to be encoded. This can be a string, a list of strings (tokenized string using\n",
      "        the `tokenize` method) or a list of integers (tokenized string ids using the `convert_tokens_to_ids`\n",
      "        method).\n",
      "\n",
      "    add_special_tokens (`bool`, *optional*, defaults to `True`):\n",
      "        Whether or not to add special tokens when encoding the sequences. This will use the underlying\n",
      "        `PretrainedTokenizerBase.build_inputs_with_special_tokens` function, which defines which tokens are\n",
      "        automatically added to the input ids. This is useful if you want to add `bos` or `eos` tokens\n",
      "        automatically.\n",
      "    padding (`bool`, `str` or [`~utils.PaddingStrategy`], *optional*, defaults to `False`):\n",
      "        Activates and controls padding. Accepts the following values:\n",
      "\n",
      "        - `True` or `'longest'`: Pad to the longest sequence in the batch (or no padding if only a single\n",
      "          sequence is provided).\n",
      "        - `'max_length'`: Pad to a maximum length specified with the argument `max_length` or to the maximum\n",
      "          acceptable input length for the model if that argument is not provided.\n",
      "        - `False` or `'do_not_pad'` (default): No padding (i.e., can output a batch with sequences of different\n",
      "          lengths).\n",
      "    truncation (`bool`, `str` or [`~tokenization_utils_base.TruncationStrategy`], *optional*, defaults to `False`):\n",
      "        Activates and controls truncation. Accepts the following values:\n",
      "\n",
      "        - `True` or `'longest_first'`: Truncate to a maximum length specified with the argument `max_length` or\n",
      "          to the maximum acceptable input length for the model if that argument is not provided. This will\n",
      "          truncate token by token, removing a token from the longest sequence in the pair if a pair of\n",
      "          sequences (or a batch of pairs) is provided.\n",
      "        - `'only_first'`: Truncate to a maximum length specified with the argument `max_length` or to the\n",
      "          maximum acceptable input length for the model if that argument is not provided. This will only\n",
      "          truncate the first sequence of a pair if a pair of sequences (or a batch of pairs) is provided.\n",
      "        - `'only_second'`: Truncate to a maximum length specified with the argument `max_length` or to the\n",
      "          maximum acceptable input length for the model if that argument is not provided. This will only\n",
      "          truncate the second sequence of a pair if a pair of sequences (or a batch of pairs) is provided.\n",
      "        - `False` or `'do_not_truncate'` (default): No truncation (i.e., can output batch with sequence lengths\n",
      "          greater than the model maximum admissible input size).\n",
      "    max_length (`int`, *optional*):\n",
      "        Controls the maximum length to use by one of the truncation/padding parameters.\n",
      "\n",
      "        If left unset or set to `None`, this will use the predefined model maximum length if a maximum length\n",
      "        is required by one of the truncation/padding parameters. If the model has no specific maximum input\n",
      "        length (like XLNet) truncation/padding to a maximum length will be deactivated.\n",
      "    stride (`int`, *optional*, defaults to 0):\n",
      "        If set to a number along with `max_length`, the overflowing tokens returned when\n",
      "        `return_overflowing_tokens=True` will contain some tokens from the end of the truncated sequence\n",
      "        returned to provide some overlap between truncated and overflowing sequences. The value of this\n",
      "        argument defines the number of overlapping tokens.\n",
      "    is_split_into_words (`bool`, *optional*, defaults to `False`):\n",
      "        Whether or not the input is already pre-tokenized (e.g., split into words). If set to `True`, the\n",
      "        tokenizer assumes the input is already split into words (for instance, by splitting it on whitespace)\n",
      "        which it will tokenize. This is useful for NER or token classification.\n",
      "    pad_to_multiple_of (`int`, *optional*):\n",
      "        If set will pad the sequence to a multiple of the provided value. Requires `padding` to be activated.\n",
      "        This is especially useful to enable the use of Tensor Cores on NVIDIA hardware with compute capability\n",
      "        `>= 7.5` (Volta).\n",
      "    padding_side (`str`, *optional*):\n",
      "        The side on which the model should have padding applied. Should be selected between ['right', 'left'].\n",
      "        Default value is picked from the class attribute of the same name.\n",
      "    return_tensors (`str` or [`~utils.TensorType`], *optional*):\n",
      "        If set, will return tensors instead of list of python integers. Acceptable values are:\n",
      "\n",
      "        - `'pt'`: Return PyTorch `torch.Tensor` objects.\n",
      "        - `'np'`: Return Numpy `np.ndarray` objects.\n",
      "\n",
      "    **kwargs: Passed along to the `.tokenize()` method.\n",
      "\n",
      "Returns:\n",
      "    `list[int]`, `torch.Tensor`, or `np.ndarray`: The tokenized ids of the text.\n",
      "\u001b[1;31mSource:\u001b[0m   \n",
      "    \u001b[1;33m@\u001b[0m\u001b[0madd_end_docstrings\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\n",
      "\u001b[0m        \u001b[0mENCODE_KWARGS_DOCSTRING\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m        \u001b[1;34m\"\"\"\n",
      "            **kwargs: Passed along to the `.tokenize()` method.\n",
      "        \"\"\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m        \u001b[1;34m\"\"\"\n",
      "        Returns:\n",
      "            `list[int]`, `torch.Tensor`, or `np.ndarray`: The tokenized ids of the text.\n",
      "        \"\"\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[1;33m)\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[1;32mdef\u001b[0m \u001b[0mencode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\n",
      "\u001b[0m        \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m        \u001b[0mtext\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTextInput\u001b[0m \u001b[1;33m|\u001b[0m \u001b[0mPreTokenizedInput\u001b[0m \u001b[1;33m|\u001b[0m \u001b[0mEncodedInput\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m        \u001b[0mtext_pair\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTextInput\u001b[0m \u001b[1;33m|\u001b[0m \u001b[0mPreTokenizedInput\u001b[0m \u001b[1;33m|\u001b[0m \u001b[0mEncodedInput\u001b[0m \u001b[1;33m|\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m        \u001b[0madd_special_tokens\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m        \u001b[0mpadding\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[1;33m|\u001b[0m \u001b[0mstr\u001b[0m \u001b[1;33m|\u001b[0m \u001b[0mPaddingStrategy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m        \u001b[0mtruncation\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[1;33m|\u001b[0m \u001b[0mstr\u001b[0m \u001b[1;33m|\u001b[0m \u001b[0mTruncationStrategy\u001b[0m \u001b[1;33m|\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m        \u001b[0mmax_length\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mint\u001b[0m \u001b[1;33m|\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m        \u001b[0mstride\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mint\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m        \u001b[0mpadding_side\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[1;33m|\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m        \u001b[0mreturn_tensors\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[1;33m|\u001b[0m \u001b[0mTensorType\u001b[0m \u001b[1;33m|\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m        \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
      "\u001b[0m        \u001b[1;34m\"\"\"\n",
      "        Converts a string to a sequence of ids (integer), using the tokenizer and vocabulary.\n",
      "\n",
      "        Same as doing `self.convert_tokens_to_ids(self.tokenize(text))`.\n",
      "\n",
      "        Args:\n",
      "            text (`str`, `list[str]` or `list[int]`):\n",
      "                The first sequence to be encoded. This can be a string, a list of strings (tokenized string using the\n",
      "                `tokenize` method) or a list of integers (tokenized string ids using the `convert_tokens_to_ids`\n",
      "                method).\n",
      "            text_pair (`str`, `list[str]` or `list[int]`, *optional*):\n",
      "                Optional second sequence to be encoded. This can be a string, a list of strings (tokenized string using\n",
      "                the `tokenize` method) or a list of integers (tokenized string ids using the `convert_tokens_to_ids`\n",
      "                method).\n",
      "        \"\"\"\u001b[0m\u001b[1;33m\n",
      "\u001b[0m        \u001b[0mpadding_strategy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtruncation_strategy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs_updated\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_padding_truncation_strategies\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\n",
      "\u001b[0m            \u001b[0mpadding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpadding\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m            \u001b[0mtruncation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtruncation\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m            \u001b[0mmax_length\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_length\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m            \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m        \u001b[1;33m)\u001b[0m\u001b[1;33m\n",
      "\u001b[0m\u001b[1;33m\n",
      "\u001b[0m        \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwargs_updated\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
      "\u001b[0m\u001b[1;33m\n",
      "\u001b[0m        \u001b[0mencoded_inputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_encode_plus\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\n",
      "\u001b[0m            \u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m            \u001b[0mtext_pair\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtext_pair\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m            \u001b[0madd_special_tokens\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0madd_special_tokens\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m            \u001b[0mpadding_strategy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpadding_strategy\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m            \u001b[0mtruncation_strategy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtruncation_strategy\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m            \u001b[0mmax_length\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_length\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m            \u001b[0mstride\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstride\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m            \u001b[0mpadding_side\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpadding_side\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m            \u001b[0mreturn_tensors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mreturn_tensors\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m            \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m        \u001b[1;33m)\u001b[0m\u001b[1;33m\n",
      "\u001b[0m\u001b[1;33m\n",
      "\u001b[0m        \u001b[1;32mreturn\u001b[0m \u001b[0mencoded_inputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"input_ids\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFile:\u001b[0m      c:\\users\\juju\\appdata\\roaming\\python\\python312\\site-packages\\transformers\\tokenization_utils_base.py\n",
      "\u001b[1;31mType:\u001b[0m      method"
     ]
    }
   ],
   "source": [
    "tokenizer.encode??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "71fb571c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[101,\n",
       " 2013,\n",
       " 1024,\n",
       " 3393,\n",
       " 2099,\n",
       " 2595,\n",
       " 3367,\n",
       " 1030,\n",
       " 11333,\n",
       " 2213,\n",
       " 1012,\n",
       " 8529,\n",
       " 2094,\n",
       " 1012,\n",
       " 3968,\n",
       " 102,\n",
       " 2013,\n",
       " 1024,\n",
       " 3124,\n",
       " 5283,\n",
       " 2080,\n",
       " 1030,\n",
       " 9806,\n",
       " 1012,\n",
       " 1057,\n",
       " 1012,\n",
       " 2899,\n",
       " 1012,\n",
       " 3968,\n",
       " 2226,\n",
       " 1006,\n",
       " 102]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(text=test_news[0],text_pair=test_news[1],max_length=32,truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d05b86ba",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "BertTokenizer has no attribute encode_plus",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[42], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m tokenizer\u001b[38;5;241m.\u001b[39mencode_plus(text\u001b[38;5;241m=\u001b[39mtest_news[\u001b[38;5;241m0\u001b[39m], text_pair\u001b[38;5;241m=\u001b[39mtest_news[\u001b[38;5;241m1\u001b[39m],max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m,truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\u001b[38;5;241m.\u001b[39mtoken_type_ids\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\transformers\\tokenization_utils_base.py:1291\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.__getattr__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   1288\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconvert_tokens_to_ids(tokens) \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;241m!=\u001b[39m key_without_id \u001b[38;5;28;01melse\u001b[39;00m tokens\n\u001b[0;32m   1290\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m:\n\u001b[1;32m-> 1291\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m has no attribute \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1292\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__getattr__\u001b[39m(key)\n",
      "\u001b[1;31mAttributeError\u001b[0m: BertTokenizer has no attribute encode_plus"
     ]
    }
   ],
   "source": [
    "tokenizer.encode_plus(text=test_news[0], text_pair=test_news[1],max_length=32,truncation=True).token_type_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "6dcc11a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] from : lerxst @ wam. umd. ed [SEP] from : guykuo @ carson. u. washington. edu ( [SEP]'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(tokenizer.encode(text=test_news[0], text_pair=test_news[1],max_length=32,truncation=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d10b6c85",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## çŸ¥è¯†ç‚¹æ€»ç»“\n",
    "\n",
    "### 1. Transformer Tokenizer æ–¹æ³•å¯¹æ¯”\n",
    "\n",
    "| æ–¹æ³• | è¿”å›ç±»å‹ | ç”¨é€” | çŠ¶æ€ |\n",
    "|------|----------|------|------|\n",
    "| `tokenizer()` / `tokenizer.__call__()` | `dict` | æ—¥å¸¸ä½¿ç”¨ï¼Œè·å–å®Œæ•´ä¿¡æ¯ | âœ… æ¨è |\n",
    "| `tokenizer.encode()` | `list` | å¿«é€Ÿè·å– token IDs | âœ… å¯ç”¨ |\n",
    "| `tokenizer.encode_plus()` | `dict` | è·å–å®Œæ•´ä¿¡æ¯ | âŒ å·²åºŸå¼ƒ |\n",
    "\n",
    "**ç¤ºä¾‹å¯¹æ¯”ï¼š**\n",
    "```python\n",
    "# tokenizer() - è¿”å›å®Œæ•´å­—å…¸\n",
    "output = tokenizer(text, text_pair=text2, max_length=32, truncation=True)\n",
    "# output.keys() â†’ ['input_ids', 'token_type_ids', 'attention_mask']\n",
    "\n",
    "# encode() - åªè¿”å› input_ids åˆ—è¡¨\n",
    "ids = tokenizer.encode(text, text_pair=text2, max_length=32, truncation=True)\n",
    "# ids â†’ [101, 2013, 1024, ...]\n",
    "```\n",
    "\n",
    "**è®¾è®¡ç†å¿µï¼š**\n",
    "- `tokenizer()`: é«˜çº§ APIï¼Œè¿”å›æ¨¡å‹éœ€è¦çš„æ‰€æœ‰ä¿¡æ¯\n",
    "- `encode()`: ç®€åŒ– APIï¼Œä¸“æ³¨æ ¸å¿ƒåŠŸèƒ½ï¼ˆæ–‡æœ¬ â†’ IDï¼‰\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Python 3 çš„ `dict.keys()` è¿”å› KeysView\n",
    "\n",
    "```python\n",
    "d = {'input_ids': [1, 2, 3], 'token_type_ids': [0, 0, 0]}\n",
    "keys = d.keys()\n",
    "print(type(keys))  # <class 'dict_keys'> (KeysView)\n",
    "```\n",
    "\n",
    "**KeysView ç‰¹æ€§ï¼š**\n",
    "- âœ… **åŠ¨æ€è§†å›¾**ï¼šåæ˜ å­—å…¸çš„å®æ—¶çŠ¶æ€\n",
    "- âœ… **åªåŒ…å«é”®**ï¼šä¸åŒ…å«å€¼ï¼Œæ”¯æŒ `len()`, `in`, è¿­ä»£\n",
    "- âœ… **å†…å­˜é«˜æ•ˆ**ï¼šä¸å¤åˆ¶æ•°æ®ï¼Œåªæ˜¯å¼•ç”¨\n",
    "- âš ï¸ **Jupyter æ˜¾ç¤º**ï¼šä¼šæ˜¾ç¤ºå®Œæ•´å­—å…¸å†…å®¹ä¾¿äºè°ƒè¯•\n",
    "\n",
    "**éªŒè¯ï¼š**\n",
    "```python\n",
    "keys = d.keys()\n",
    "print(list(keys))  # ['input_ids', 'token_type_ids']\n",
    "d['new_key'] = 'value'\n",
    "print(list(keys))  # ['input_ids', 'token_type_ids', 'new_key']  # è§†å›¾è‡ªåŠ¨æ›´æ–°\n",
    "```\n",
    "\n",
    "**å¦‚æœä½ åªæƒ³è¦é”®çš„åˆ—è¡¨ï¼š**\n",
    "```python\n",
    "keys_list = list(tokenizer(...).keys())\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Tokenizer è¾“å‡ºå­—æ®µè¯´æ˜\n",
    "\n",
    "| å­—æ®µ | å«ä¹‰ | è¯´æ˜ |\n",
    "|------|------|------|\n",
    "| `input_ids` | Token ID åºåˆ— | æ–‡æœ¬è½¬æ¢æˆçš„æ•°å­—ç´¢å¼• |\n",
    "| `token_type_ids` | å¥å­ç±»å‹æ ‡è¯† | 0=ç¬¬ä¸€å¥ï¼Œ1=ç¬¬äºŒå¥ï¼ˆç”¨äº NSP ä»»åŠ¡ï¼‰ |\n",
    "| `attention_mask` | æ³¨æ„åŠ›æ©ç  | 1=çœŸå® tokenï¼Œ0=padding token |\n",
    "\n",
    "**æ³¨æ„ï¼š**\n",
    "- å•å¥å­è¾“å…¥æ—¶ï¼Œ`token_type_ids` å…¨éƒ¨ä¸º 0\n",
    "- å¥å­å¯¹è¾“å…¥æ—¶ï¼Œç¬¬ä¸€å¥ä¸º 0ï¼Œç¬¬äºŒå¥ä¸º 1\n",
    "- ä½¿ç”¨ `tokenizer()` æ—¶é»˜è®¤æ‰€æœ‰å¥å­ä¸º 0\n",
    "- ä½¿ç”¨ `encode()` æŒ‡å®š `text_pair` æ—¶ä¼šè‡ªåŠ¨è®¾ç½® `token_type_ids`\n",
    "\n",
    "---\n",
    "\n",
    "### 4. è¿ç§»æŒ‡å—ï¼ˆæ—§ä»£ç  â†’ æ–°ä»£ç ï¼‰\n",
    "\n",
    "```python\n",
    "# âŒ æ—§ä»£ç ï¼ˆä¸å†å·¥ä½œï¼‰\n",
    "output = tokenizer.encode_plus(text, text_pair=text2)\n",
    "ids = output['input_ids']\n",
    "\n",
    "# âœ… æ–°ä»£ç \n",
    "output = tokenizer(text, text_pair=text2)  # æ¨è\n",
    "ids = output.input_ids\n",
    "\n",
    "# æˆ–\n",
    "ids = tokenizer.encode(text, text_pair=text2)  # ç®€å•åœºæ™¯\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“ 2022 å¹´æ•™ç¨‹ â†’ 2025 å¹´æ›´æ–°è¦ç‚¹\n",
    "\n",
    "### å·²åºŸå¼ƒçš„ API\n",
    "- âŒ `tokenizer.encode_plus()` â†’ âœ… ä½¿ç”¨ `tokenizer()`\n",
    "- âŒ ç›´æ¥è®¿é—®è¿”å›å­—å…¸çš„ `.token_type_ids` å±æ€§åœ¨ `encode_plus` ä¸­å¤±æ•ˆ\n",
    "\n",
    "### æ–°å¢æ¨èå‚æ•°\n",
    "\n",
    "#### 1. `return_tensors` - è¿”å›æ¡†æ¶å¼ é‡\n",
    "```python\n",
    "# è¿”å› PyTorch å¼ é‡\n",
    "output = tokenizer(text, return_tensors='pt')\n",
    "# output.input_ids æ˜¯ torch.Tensor\n",
    "\n",
    "# è¿”å› TensorFlow å¼ é‡\n",
    "output = tokenizer(text, return_tensors='tf')\n",
    "# è¿”å› NumPy æ•°ç»„\n",
    "output = tokenizer(text, return_tensors='np')\n",
    "```\n",
    "\n",
    "#### 2. `padding` - å¡«å……ç­–ç•¥\n",
    "```python\n",
    "# åŠ¨æ€å¡«å……åˆ°æ‰¹æ¬¡ä¸­æœ€é•¿åºåˆ—\n",
    "tokenizer(batch_texts, padding=True)\n",
    "\n",
    "# å¡«å……åˆ°æŒ‡å®šé•¿åº¦\n",
    "tokenizer(batch_texts, padding='max_length', max_length=128)\n",
    "\n",
    "# ä¸å¡«å……ï¼ˆé»˜è®¤ï¼‰\n",
    "tokenizer(batch_texts, padding=False)\n",
    "```\n",
    "\n",
    "#### 3. `truncation` - æˆªæ–­ç­–ç•¥\n",
    "```python\n",
    "# æˆªæ–­åˆ°æ¨¡å‹æœ€å¤§é•¿åº¦\n",
    "tokenizer(long_text, truncation=True)\n",
    "\n",
    "# æˆªæ–­åˆ°æŒ‡å®šé•¿åº¦\n",
    "tokenizer(long_text, truncation=True, max_length=128)\n",
    "\n",
    "# åªæˆªæ–­ç¬¬ä¸€å¥\n",
    "tokenizer(text, text_pair=text2, truncation='only_first')\n",
    "```\n",
    "\n",
    "#### 4. `add_special_tokens` - æ§åˆ¶ç‰¹æ®Š token\n",
    "```python\n",
    "# é»˜è®¤æ·»åŠ  [CLS] å’Œ [SEP]\n",
    "tokenizer(text)  # æ·»åŠ ç‰¹æ®Š tokens\n",
    "\n",
    "# ä¸æ·»åŠ ç‰¹æ®Š tokens\n",
    "tokenizer(text, add_special_tokens=False)\n",
    "```\n",
    "\n",
    "### æ‰¹å¤„ç†æœ€ä½³å®è·µ\n",
    "\n",
    "```python\n",
    "# âœ… æ¨èï¼šè‡ªåŠ¨å¤„ç† padding å’Œ truncation\n",
    "batch_output = tokenizer(\n",
    "    ['text 1', 'text 2', 'text 3'],\n",
    "    padding=True,          # è‡ªåŠ¨å¡«å……åˆ°æ‰¹æ¬¡æœ€é•¿\n",
    "    truncation=True,       # è‡ªåŠ¨æˆªæ–­è¶…é•¿æ–‡æœ¬\n",
    "    max_length=512,        # æœ€å¤§é•¿åº¦\n",
    "    return_tensors='pt'    # è¿”å› PyTorch å¼ é‡\n",
    ")\n",
    "\n",
    "# è®¿é—®ç»“æœ\n",
    "input_ids = batch_output.input_ids          # shape: [3, max_len]\n",
    "attention_mask = batch_output.attention_mask\n",
    "```\n",
    "\n",
    "### å­—å…¸è®¿é—®æ–¹å¼ï¼ˆä¸¤ç§éƒ½æ”¯æŒï¼‰\n",
    "\n",
    "```python\n",
    "result = tokenizer(text)\n",
    "\n",
    "# æ–¹å¼ 1ï¼šç‚¹å·è®¿é—®ï¼ˆæ¨èï¼Œæ›´ç®€æ´ï¼‰\n",
    "ids = result.input_ids\n",
    "mask = result.attention_mask\n",
    "\n",
    "# æ–¹å¼ 2ï¼šå­—å…¸è®¿é—®ï¼ˆå…¼å®¹æ€§å¼ºï¼‰\n",
    "ids = result['input_ids']\n",
    "mask = result['attention_mask']\n",
    "```\n",
    "\n",
    "### å¸¸ç”¨å‚æ•°ç»„åˆé€ŸæŸ¥\n",
    "\n",
    "```python\n",
    "# ğŸ”¥ æ—¥å¸¸ä½¿ç”¨ï¼ˆæ¨èï¼‰\n",
    "tokenizer(text, padding='longest', return_tensors='pt')\n",
    "\n",
    "# ğŸ”¥ å•æ¡æ–‡æœ¬ï¼Œä¸å¡«å……\n",
    "tokenizer(text, truncation=True)\n",
    "\n",
    "# ğŸ”¥ æ‰¹é‡å¤„ç†ï¼Œå›ºå®šé•¿åº¦\n",
    "tokenizer(batch_texts, padding=True, truncation=True, max_length=128, return_tensors='pt')\n",
    "\n",
    "# ğŸ”¥ å¥å­å¯¹ï¼ˆå¦‚é—®ç­”ä»»åŠ¡ï¼‰\n",
    "tokenizer(question, text_pair=passage, padding=True, truncation=True)\n",
    "\n",
    "# ğŸ”¥ ä¸æ·»åŠ ç‰¹æ®Š tokenï¼ˆæŸäº›ç‰¹æ®Šåœºæ™¯ï¼‰\n",
    "tokenizer(text, add_special_tokens=False)\n",
    "```\n",
    "\n",
    "### æ³¨æ„äº‹é¡¹\n",
    "\n",
    "1. **ç‰ˆæœ¬å…¼å®¹æ€§**ï¼š2022 å¹´æ•™ç¨‹ä¸­çš„ `encode_plus` åœ¨ 2025+ ç‰ˆæœ¬ä¸­å·²å®Œå…¨ç§»é™¤\n",
    "2. **é»˜è®¤è¡Œä¸ºå˜åŒ–**ï¼šæ–°ç‰ˆæœ¬é»˜è®¤ä¸å¯ç”¨ paddingï¼Œéœ€è¦æ˜¾å¼æŒ‡å®š `padding=True`\n",
    "3. **æ€§èƒ½ä¼˜åŒ–**ï¼šæ‰¹å¤„ç†æ—¶æ¨èä¸€æ¬¡æ€§ä¼ å…¥åˆ—è¡¨ï¼Œè€Œä¸æ˜¯å¾ªç¯è°ƒç”¨\n",
    "4. **æ¡†æ¶é›†æˆ**ï¼šä½¿ç”¨ `return_tensors='pt'` å¯ç›´æ¥è¿”å›æ¨¡å‹è¾“å…¥æ ¼å¼\n",
    "\n",
    "### æ¨èå­¦ä¹ è·¯å¾„\n",
    "\n",
    "```python\n",
    "# 1ï¸âƒ£ åŸºç¡€ä½¿ç”¨\n",
    "tokenizer(text)\n",
    "\n",
    "# 2ï¸âƒ£ æ·»åŠ é•¿åº¦æ§åˆ¶\n",
    "tokenizer(text, truncation=True, max_length=512)\n",
    "\n",
    "# 3ï¸âƒ£ æ‰¹å¤„ç†\n",
    "tokenizer(batch_texts, padding=True, truncation=True)\n",
    "\n",
    "# 4ï¸âƒ£ æ¨¡å‹å°±ç»ªæ ¼å¼\n",
    "tokenizer(batch_texts, padding=True, truncation=True, return_tensors='pt')\n",
    "\n",
    "# 5ï¸âƒ£ å®Œæ•´æµç¨‹\n",
    "inputs = tokenizer(text, return_tensors='pt')\n",
    "outputs = model(**inputs)\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
